{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c9ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e7fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 43835 examples [00:00, 244163.83 examples/s]\n",
      "Generating test split: 2354 examples [00:00, 163667.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#change data_dir for the subfolder you want.\n",
    "data_cache = '/Users/august/Documents/LLM Research/new_self_play_drpo/data_cache'\n",
    "ds = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"helpful-base\", cache_dir=data_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e19802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['chosen', 'rejected'],\n",
      "        num_rows: 43835\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['chosen', 'rejected'],\n",
      "        num_rows: 2354\n",
      "    })\n",
      "})\n",
      "---\n",
      "{'chosen': '\\n\\nHuman: Do you know why turkeys became the official food of thanksgiving?\\n\\nAssistant: To be honest, I don’t know anything about that. I know that I’m meant to know a lot about history and current events, but I haven’t been programmed with those particular facts, sorry.', 'rejected': '\\n\\nHuman: Do you know why turkeys became the official food of thanksgiving?\\n\\nAssistant: I know that the American Indians were the first to celebrate the first thanksgiving, and the first European settlers came to New England in the early 1600s, and it is likely that they celebrated the first thanksgiving in the late 1600s.  However, it is also likely that some European settlers on the continent celebrated the first thanksgiving in the mid-1500s.  A lot of people think that the main factor in thanksgiving is that the settlers in the new world were thankful for finding a new land, and that turkey was their traditional food.  Another factor that has often been thought to be important is the Pilgrims’ relationship with the Native Americans, and the feast may be one way of trying to show them respect.  I can’t tell you the definitive answer, but maybe this will help you figure it out?'}\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print(\"---\")\n",
    "print(ds['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f7ae2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset: 46189 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 46189/46189 [00:00<00:00, 47359.88 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['chosen', 'rejected', 'prompt'],\n",
      "    num_rows: 46189\n",
      "})\n",
      "---\n",
      "Prompt conversation:\n",
      "  [user]: Hi, I want to learn to play horseshoes. Can you teach me?\n",
      "  [assistant]: I can, but maybe I should begin by telling you that a typical game consists of 2 players and 6 or 8 \n",
      "  [user]: Okay. What else is needed to play, and what are the rules?\n",
      "Chosen:\n",
      "  [assistant]: A horseshoe is usually made out of metal and is about 3 to 3.5 inches long and around 1 inch thick. The horseshoe should also have a 2 inch by 3 inch flat at the bottom where the rubber meets the metal. We also need two stakes and six horseshoes.\n",
      "Rejected:\n",
      "  [assistant]: Horseshoes are either metal or plastic discs. The horseshoes come in different weights, and the lighter ones are easier to throw, so they are often the standard for beginning players.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, Dataset\n",
    "import re\n",
    "\n",
    "def parse_conversation(text):\n",
    "    \"\"\"Parse a raw conversation string into a list of {role, content} dicts.\"\"\"\n",
    "    # Normalize: ensure text starts with \\n\\n so the split pattern is uniform\n",
    "    text = text.strip()\n",
    "    if not text.startswith(\"\\n\\n\"):\n",
    "        text = \"\\n\\n\" + text\n",
    "    # Split on \\n\\nHuman: or \\n\\nAssistant: keeping the role label\n",
    "    parts = re.split(r'\\n\\n(Human|Assistant):\\s*', text)\n",
    "    conversation = []\n",
    "    # parts: ['', 'Human', 'msg1', 'Assistant', 'msg2', ...]\n",
    "    i = 1\n",
    "    while i < len(parts) - 1:\n",
    "        role = \"user\" if parts[i] == \"Human\" else \"assistant\"\n",
    "        content = parts[i + 1].strip()\n",
    "        if content:\n",
    "            conversation.append({\"role\": role, \"content\": content})\n",
    "        i += 2\n",
    "    return conversation\n",
    "\n",
    "def extract_prompt_chosen_rejected(example):\n",
    "    \"\"\"Extract shared prompt, chosen-only response, rejected-only response.\"\"\"\n",
    "    chosen_conv = parse_conversation(example[\"chosen\"])\n",
    "    rejected_conv = parse_conversation(example[\"rejected\"])\n",
    "    \n",
    "    # Find the common prefix (the prompt turns)\n",
    "    common_len = 0\n",
    "    for c, r in zip(chosen_conv, rejected_conv):\n",
    "        if c == r:\n",
    "            common_len += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    prompt = chosen_conv[:common_len]\n",
    "    chosen = chosen_conv[common_len:]\n",
    "    rejected = rejected_conv[common_len:]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# 1. Combine train and test splits\n",
    "combined = concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n",
    "print(f\"Combined dataset: {len(combined)} rows\")\n",
    "\n",
    "# 2. Reformat: extract prompt, chosen, rejected in conversation format\n",
    "combined = combined.map(extract_prompt_chosen_rejected)\n",
    "\n",
    "print(combined)\n",
    "print(\"---\")\n",
    "# Verify first example\n",
    "ex = combined[0]\n",
    "print(\"Prompt conversation:\")\n",
    "for turn in ex[\"prompt\"]:\n",
    "    print(f\"  [{turn['role']}]: {turn['content'][:100]}\")\n",
    "print(\"Chosen:\")\n",
    "for turn in ex[\"chosen\"]:\n",
    "    print(f\"  [{turn['role']}]: {turn['content']}\")\n",
    "print(\"Rejected:\")\n",
    "for turn in ex[\"rejected\"]:\n",
    "    print(f\"  [{turn['role']}]: {turn['content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a595a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  7.98ba/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 26.5MB / 26.5MB,  319kB/s  \n",
      "New Data Upload: 100%|██████████|  256kB /  256kB,  319kB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.64s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/august66/hh_helpful_base/commit/e9d75bb12fd9f81a5f7074e933bb5ced049529c7', commit_message='Upload dataset', commit_description='', oid='e9d75bb12fd9f81a5f7074e933bb5ced049529c7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/august66/hh_helpful_base', endpoint='https://huggingface.co', repo_type='dataset', repo_id='august66/hh_helpful_base'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = combined.select_columns(['prompt', 'chosen', 'rejected'])\n",
    "hf_name = 'august66/hh_helpful_base'\n",
    "combined.push_to_hub(hf_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872af336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
