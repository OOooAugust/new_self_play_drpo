{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe98f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys, pathlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification,DebertaV2ForSequenceClassification, GPTNeoXForCausalLM\n",
    "from llm_blender.pair_ranker.pairrm import DebertaV2PairRM\n",
    "from transformers import DataCollatorWithPadding\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "LOCAL_TRL_PARENT = \"/workspace/Self_play_DRPO\"\n",
    "if LOCAL_TRL_PARENT not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_TRL_PARENT)\n",
    "\n",
    "    \n",
    "# now the import will use your local copy:\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    ModelConfig,\n",
    "    DRPOTrainer,\n",
    "    DRPOConfig,\n",
    ")\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "from trl.data_utils import apply_chat_template\n",
    "from trl.trainer.drpo_utils import PairRMPipeline\n",
    "\n",
    "def process_split(original, seed = 42):\n",
    "    swapped = original.map(lambda x: {\n",
    "        'a1': x['a2'],\n",
    "        'a2': x['a1'],\n",
    "        # 'rank': 1 - int(random.random() < x['chosen_preference']),\n",
    "        'rank': 1 - x['rank'],\n",
    "    })\n",
    "\n",
    "    return concatenate_datasets([original, swapped]).shuffle(seed=seed)\n",
    "\n",
    "def load_model(model_path, task = 'generation', model_type = 'decoder', model_cache_path =  '/workspace/model_cache'):\n",
    "\n",
    "    model_args = ModelConfig(model_path)\n",
    "    model_torch_dtype = torch.bfloat16\n",
    "    model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "    padding_side = 'left' if model_type == 'decoder' else 'right'\n",
    "    truncation_side = 'left' if model_type == 'decoder' else 'right'\n",
    "\n",
    "    if task == 'generation':\n",
    "        model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "\n",
    "    elif task == 'reward':\n",
    "        model_instance = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "    \n",
    "\n",
    "    model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, \n",
    "        padding_side = padding_side, \n",
    "        truncation_side = truncation_side,\n",
    "        use_fast = True,\n",
    "        trust_remote_code = model_args.trust_remote_code,\n",
    "        cache_dir = model_cache_path\n",
    "    )\n",
    "\n",
    "    if model_tokenizer.pad_token is None:\n",
    "        model_tokenizer.pad_token = model_tokenizer.eos_token\n",
    "\n",
    "    if getattr(model_instance.config, \"pad_token_id\", None) is None:\n",
    "        model_instance.config.pad_token_id = model_tokenizer.pad_token_id\n",
    "\n",
    "    if model_tokenizer.eos_token is None:\n",
    "        model_tokenizer.eos_token = model_tokenizer.pad_token  \n",
    "\n",
    "    if getattr(model_instance.config, \"eos_token_id\", None) is None:\n",
    "        model_instance.config.eos_token_id = model_tokenizer.eos_token_id\n",
    "\n",
    "    return model_instance, model_tokenizer\n",
    "\n",
    "\n",
    "\n",
    "data_cache_path = \"/workspace/dataset\"\n",
    "model_cache_path = '/workspace/model_cache'\n",
    "ds_path = 'august66/hh_qwen2.5_1.5b_with_bias'\n",
    "ref_policy_path = \"Qwen/Qwen2.5-1.5B-Instruct\" \n",
    "target_policy_path = \"Qwen/Qwen2.5-1.5B-Instruct\" \n",
    "dpo_policy_path = 'august66/hh_qwen_1.5b_dpo_model_2'\n",
    "train_split =  'train_chunk_00000'\n",
    "\n",
    "#load training argument for drpo\n",
    "with open(\"/workspace/Self_play_DRPO/self_play_drpo_code/training_config/config_normal_dist.yaml\", \"r\") as f:\n",
    "    training_args_config = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1bbb2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DRPOConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m training_args = \u001b[43mDRPOConfig\u001b[49m(\n\u001b[32m      2\u001b[39m     **training_args_config\n\u001b[32m      3\u001b[39m )\n\u001b[32m      4\u001b[39m seed = \u001b[32m1234\u001b[39m\n\u001b[32m      5\u001b[39m ref_policy_model, ref_policy_tokenizer = load_model(ref_policy_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'DRPOConfig' is not defined"
     ]
    }
   ],
   "source": [
    "training_args = DRPOConfig(\n",
    "    **training_args_config\n",
    ")\n",
    "seed = 1234\n",
    "ref_policy_model, ref_policy_tokenizer = load_model(ref_policy_path)\n",
    "target_policy_model, target_policy_tokenizer = load_model(target_policy_path)\n",
    "dpo_policy_model, dpo_policy_tokenizer = load_model(dpo_policy_path)\n",
    "preference_model, preference_model_tokenizer = load_model(training_args.preference_model_id, task = 'reward')\n",
    "drpo_train = load_dataset(ds_path, cache_dir=data_cache_path, split = train_split)\n",
    "\n",
    "\n",
    "trainer = DRPOTrainer(\n",
    "    model=target_policy_model,\n",
    "    ref_model=ref_policy_model,\n",
    "    dpo_model = dpo_policy_model,\n",
    "    preference_model=preference_model,\n",
    "    train_dataset = drpo_train,\n",
    "    processing_class=ref_policy_tokenizer,\n",
    "    args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "328428de",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "training_args = DRPOConfig(\n",
    "    **training_args_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb706217",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model, tokenizer = load_model(target_policy_path)\n",
    "ref_model, tokenizer = load_model(target_policy_path)\n",
    "target_model.to('cuda')\n",
    "ref_model.to('cuda')\n",
    "drpo_train = load_dataset(ds_path, cache_dir=data_cache_path, split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ae098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "You will be given a definition of a task first, then some input of the task.\n",
      "In this task, you are given a sentence which is either in the Gujarati language or English language. You task is to identify the language of input sentence. Input sentence can be in Gujarari or English language only and also it cannot have two languages at a time.\n",
      "\n",
      "એક ટ્રેન એન્જિન કે જે રેલરોડ ટ્રેક પર મુસાફરી કરેલા ધૂમ્રપાનની સાથે જોડાયેલી છે તેમાં ઘણી પેસેન્જર કાર જોડાયેલી છે.\n",
      "Output:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nYou will be given a definition of a task first, then some input of the task.\\nIn this task, you are given a sentence which is either in the Gujarati language or English language. You task is to identify the language of input sentence. Input sentence can be in Gujarari or English language only and also it cannot have two languages at a time.\\n\\nએક ટ્રેન એન્જિન કે જે રેલરોડ ટ્રેક પર મુસાફરી કરેલા ધૂમ્રપાનની સાથે જોડાયેલી છે તેમાં ઘણી પેસેન્જર કાર જોડાયેલી છે.\\nOutput:<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "print (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb6e0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors = 'pt',\n",
    "    truncation = True,\n",
    "    max_length = 1024\n",
    ").to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d69e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "outputs_1 = ref_model.generate(\n",
    "    **enc,\n",
    "    do_sample = False, \n",
    "    max_new_tokens = 1024,\n",
    "    temperature = 0\n",
    ")\n",
    "outputs_2 = target_model.generate(\n",
    "    **enc,\n",
    "    do_sample = False, \n",
    "    max_new_tokens = 1024,\n",
    "    temperature = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8ae88d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
