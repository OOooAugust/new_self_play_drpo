{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2195eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys, pathlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification,DebertaV2ForSequenceClassification, GPTNeoXForCausalLM\n",
    "from llm_blender.pair_ranker.pairrm import DebertaV2PairRM\n",
    "from transformers import DataCollatorWithPadding\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "LOCAL_TRL_PARENT = \"/workspace/Self_play_DRPO\"\n",
    "if LOCAL_TRL_PARENT not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_TRL_PARENT)\n",
    "\n",
    "    \n",
    "# now the import will use your local copy:\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    ModelConfig,\n",
    "    DRPOTrainer,\n",
    "    DRPOConfig,\n",
    ")\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "from trl.trainer.utils import pad, truncate_right, selective_log_softmax\n",
    "from trl.data_utils import apply_chat_template\n",
    "\n",
    "data_cache_path = \"/workspace/dataset\"\n",
    "model_cache_path = '/workspace/model_cache'\n",
    "ref_policy_path = \"Qwen/Qwen2.5-1.5B-Instruct\" \n",
    "#target_policy_path = \"Qwen/Qwen2.5-1.5B-Instruct\" \n",
    "target_policy_400_path = 'august66/hh_qwen1.5_drpo_400'\n",
    "target_policy_800_path = 'august66/hh_qwen1.5_drpo_800'\n",
    "dpo_policy_path = 'august66/hh_qwen_1.5b_dpo_model_2'\n",
    "#reward_model_path = 'cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr'\n",
    "ds_path = 'august66/drpo_hh_qwen2.5_1.5b'\n",
    "\n",
    "\n",
    "def load_model(model_path, task = 'generation', model_type = 'decoder', model_cache_path =  '/workspace/model_cache'):\n",
    "\n",
    "    model_args = ModelConfig(model_path)\n",
    "    #model_torch_dtype = (model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype))\n",
    "    model_torch_dtype = torch.bfloat16 \n",
    "    model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "    padding_side = 'left' if model_type == 'decoder' else 'right'\n",
    "    truncation_side = 'left' if model_type == 'decoder' else 'right'\n",
    "\n",
    "    if task == 'generation':\n",
    "        model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "\n",
    "    elif task == 'reward':\n",
    "        model_instance = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "    \n",
    "\n",
    "    model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, \n",
    "        padding_side = padding_side, \n",
    "        truncation_side = truncation_side,\n",
    "        use_fast = True,\n",
    "        trust_remote_code = model_args.trust_remote_code,\n",
    "        cache_dir = model_cache_path\n",
    "    )\n",
    "\n",
    "    if model_tokenizer.pad_token is None:\n",
    "        model_tokenizer.pad_token = model_tokenizer.eos_token\n",
    "\n",
    "    if getattr(model_instance.config, \"pad_token_id\", None) is None:\n",
    "        model_instance.config.pad_token_id = model_tokenizer.pad_token_id\n",
    "\n",
    "    if model_tokenizer.eos_token is None:\n",
    "        model_tokenizer.eos_token = model_tokenizer.pad_token  \n",
    "\n",
    "    if getattr(model_instance.config, \"eos_token_id\", None) is None:\n",
    "        model_instance.config.eos_token_id = model_tokenizer.eos_token_id\n",
    "\n",
    "    return model_instance, model_tokenizer\n",
    "\n",
    "\n",
    "def generate_responses(\n",
    "    model_instance,\n",
    "    model_tokenizer,\n",
    "    prompts,                      # single chat or list of chats (each: list[{role,content}])\n",
    "    *,\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=256,\n",
    "    n_responses_total=1000,       # total per prompt\n",
    "    responses_per_call=16,        # per subcall per prompt\n",
    "    batch_size=4,                 # prompts per batch\n",
    "    device='cuda',\n",
    "    max_length=1024,\n",
    "):\n",
    "    # tokenizer prefs\n",
    "    model_tokenizer.padding_side = \"left\"\n",
    "    if hasattr(model_tokenizer, \"truncation_side\"):\n",
    "        model_tokenizer.truncation_side = \"left\"\n",
    "    if model_tokenizer.pad_token_id is None and model_tokenizer.eos_token_id is not None:\n",
    "        model_tokenizer.pad_token_id = model_tokenizer.eos_token_id\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"do_sample\": bool(temperature > 0),\n",
    "        \"temperature\": float(temperature),\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"eos_token_id\": model_tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": model_tokenizer.pad_token_id,\n",
    "        \"use_cache\": True,\n",
    "        \"return_dict_in_generate\": False,\n",
    "        \"output_scores\": False,\n",
    "        \"output_attentions\": False,\n",
    "        \"output_hidden_states\": False,\n",
    "    }\n",
    "\n",
    "    # normalize prompts input\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "    elif (isinstance(prompts, list) and prompts\n",
    "          and isinstance(prompts[0], dict) and \"role\" in prompts[0]):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    model_instance.to(device).eval()\n",
    "\n",
    "    prompts_out, completions_out = [], []\n",
    "\n",
    "    for s in range(0, len(prompts), batch_size):\n",
    "        batch_prompts_raw = prompts[s:s+batch_size]\n",
    "\n",
    "        # render once per batch\n",
    "        batch_rendered = [\n",
    "            model_tokenizer.apply_chat_template(p, add_generation_prompt=True, tokenize=False)\n",
    "            for p in batch_prompts_raw\n",
    "        ]\n",
    "        enc = model_tokenizer(\n",
    "            batch_rendered, padding=True, truncation=True,\n",
    "            max_length=max_length, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        T_in = enc[\"input_ids\"].size(1)\n",
    "\n",
    "        # bucket completions per prompt index within this batch\n",
    "        per_prompt_bucket = [[] for _ in range(len(batch_prompts_raw))]\n",
    "\n",
    "        remaining = n_responses_total\n",
    "        while remaining > 0:\n",
    "            cur_n = min(responses_per_call, remaining)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_ids = model_instance.generate(\n",
    "                    **enc, num_return_sequences=cur_n, **gen_kwargs\n",
    "                )  # [B*cur_n, T_in + gen_len]\n",
    "\n",
    "            comp_ids = out_ids[:, T_in:]\n",
    "            decoded = model_tokenizer.batch_decode(comp_ids, skip_special_tokens=True)\n",
    "\n",
    "            # outputs are stacked: p0 x cur_n, p1 x cur_n, ...\n",
    "            total = comp_ids.size(0)  # B*cur_n\n",
    "            for k in range(total):\n",
    "                base_i = k // cur_n  # which prompt in this subcall\n",
    "                per_prompt_bucket[base_i].append(decoded[k])\n",
    "\n",
    "            # free between subcalls\n",
    "            del out_ids, comp_ids, decoded\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            remaining -= cur_n\n",
    "\n",
    "        # now FLATTEN in prompt order so each prompt's 1..N completions are contiguous\n",
    "        for i, completions in enumerate(per_prompt_bucket):\n",
    "            for c in completions:\n",
    "                prompts_out.append(batch_prompts_raw[i])  # original chat object\n",
    "                completions_out.append(c)\n",
    "\n",
    "        del enc\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return Dataset.from_dict({\"prompt\": prompts_out, \"completion\": completions_out})\n",
    "\n",
    "def get_reward_batch(\n",
    "    model_instance, \n",
    "    model_tokenizer, \n",
    "    inputs,                    # list[(prompt, response)]\n",
    "    device='cuda', \n",
    "    batch_size=32\n",
    "):\n",
    "    model_instance.to(device)\n",
    "    all_rewards = []\n",
    "\n",
    "    for start in tqdm(range(0, len(inputs), batch_size)):\n",
    "        batch_inputs = inputs[start:start+batch_size]\n",
    "\n",
    "        # concat prompt+response text\n",
    "        responses = [prompt + response for (prompt, response) in batch_inputs]\n",
    "\n",
    "        encoded_inputs = model_tokenizer(\n",
    "            responses,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_instance(**encoded_inputs)\n",
    "\n",
    "        # Typical reward heads return shape [B] or [B,1] in outputs.logits\n",
    "        rewards = outputs.logits.squeeze(-1).detach().cpu().tolist()\n",
    "        if isinstance(rewards, list):\n",
    "            all_rewards.extend(rewards)\n",
    "        else:\n",
    "            all_rewards.append(rewards)\n",
    "\n",
    "        del encoded_inputs, outputs\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch,                         # dict with batch['prompt'], batch['completion']\n",
    "    *,\n",
    "    temperature=1.0,\n",
    "    max_length=1024,\n",
    "    add_generation_prompt=True,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Single batched pass. Returns tensor [B] with sum log-prob of each completion\n",
    "    conditioned on its prompt.\n",
    "    \"\"\"\n",
    "    prompts      = batch[\"prompt\"]\n",
    "    completions  = [str(c) for c in batch[\"completion\"]]\n",
    "    if len(completions) == 0:\n",
    "        return torch.empty(0)\n",
    "\n",
    "    # Render chat prompts where needed\n",
    "    def render(p):\n",
    "        if isinstance(p, list) and p and isinstance(p[0], dict) and \"role\" in p[0]:\n",
    "            return tokenizer.apply_chat_template(p, add_generation_prompt=add_generation_prompt, tokenize=False)\n",
    "        return str(p)\n",
    "    prompts_text = [render(p) for p in prompts]\n",
    "\n",
    "    # Temporarily force LEFT padding so (T_p_max - 1) is a real token\n",
    "    saved_pad_side = getattr(tokenizer, \"padding_side\", \"right\")\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # Tokenize prompt & completion (batched)\n",
    "    enc_p = tokenizer(prompts_text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "    enc_c = tokenizer(completions,  padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Concat (teacher forcing)\n",
    "    input_ids = torch.cat([enc_p[\"input_ids\"], enc_c[\"input_ids\"]], dim=1)\n",
    "    attn_mask = torch.cat([enc_p[\"attention_mask\"], enc_c[\"attention_mask\"]], dim=1)\n",
    "\n",
    "    # Forward once\n",
    "    logits = model(input_ids=input_ids, attention_mask=attn_mask).logits  # [B, T, V]\n",
    "    if temperature and temperature > 0:\n",
    "        logits = logits / (temperature + 1e-7)\n",
    "\n",
    "    # Align to completion window (next-token prediction)\n",
    "    T_p = enc_p[\"input_ids\"].size(1)\n",
    "    T_c = enc_c[\"input_ids\"].size(1)\n",
    "    start = T_p - 1\n",
    "    end   = min(start + T_c, logits.size(1))\n",
    "    T_eff = end - start\n",
    "\n",
    "    win_logits = logits[:, start:end, :]                         # [B, T_eff, V]\n",
    "    tgt_ids = enc_c[\"input_ids\"][:, :T_eff]                      # [B, T_eff]\n",
    "    tgt_msk = enc_c[\"attention_mask\"][:, :T_eff].to(torch.bool)  # [B, T_eff]\n",
    "\n",
    "    # Your original scoring helper\n",
    "    logps  = selective_log_softmax(win_logits, tgt_ids)          # [B, T_eff]\n",
    "    scores = (logps * tgt_msk).sum(dim=1)                        # [B]\n",
    "\n",
    "    # Restore tokenizer padding side\n",
    "    tokenizer.padding_side = saved_pad_side\n",
    "    \n",
    "    del win_logits, tgt_ids, tgt_msk, logits\n",
    "    return scores.detach().cpu()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_expected_kl_batch(\n",
    "    ref_model,\n",
    "    tokenizer,                     # same tokenizer for both models\n",
    "    target_model,\n",
    "    batch,                         # dict with batch['prompt'], batch['completion']\n",
    "    *,\n",
    "    max_length=1024,\n",
    "    add_generation_prompt=True,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Single batched pass. Returns python list [B] with KL per example over completion tokens.\n",
    "    \"\"\"\n",
    "    prompts     = batch[\"prompt\"]\n",
    "    completions = [str(c) for c in batch[\"completion\"]]\n",
    "    if len(completions) == 0:\n",
    "        return []\n",
    "\n",
    "    # Render prompts\n",
    "    def render(p):\n",
    "        if isinstance(p, list) and p and isinstance(p[0], dict) and \"role\" in p[0]:\n",
    "            return tokenizer.apply_chat_template(p, add_generation_prompt=add_generation_prompt, tokenize=False)\n",
    "        return str(p)\n",
    "    prompts_text = [render(p) for p in prompts]\n",
    "\n",
    "    # LEFT padding\n",
    "    saved_pad_side = getattr(tokenizer, \"padding_side\", \"right\")\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    ref_model    = ref_model.to(device).eval()\n",
    "    target_model = target_model.to(device).eval()\n",
    "\n",
    "    # Tokenize\n",
    "    enc_p = tokenizer(prompts_text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "    enc_c = tokenizer(completions,  padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Concat\n",
    "    input_ids = torch.cat([enc_p[\"input_ids\"], enc_c[\"input_ids\"]], dim=1)\n",
    "    attn_mask = torch.cat([enc_p[\"attention_mask\"], enc_c[\"attention_mask\"]], dim=1)\n",
    "\n",
    "    # Forwards (batched)\n",
    "    ref_logits    = ref_model(input_ids=input_ids,    attention_mask=attn_mask).logits\n",
    "    target_logits = target_model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
    "\n",
    "    # Slice to completion region (next-token prediction)\n",
    "    T_p = enc_p[\"input_ids\"].size(1)\n",
    "    T_c = enc_c[\"input_ids\"].size(1)\n",
    "    start = T_p - 1\n",
    "    end   = min(start + T_c, target_logits.size(1))\n",
    "    T_eff = end - start\n",
    "\n",
    "    ref_logp = F.log_softmax(ref_logits[:,    start:end, :], dim=-1)   # [B, T_eff, V]\n",
    "    tgt_logp = F.log_softmax(target_logits[:, start:end, :], dim=-1)   # [B, T_eff, V]\n",
    "    ref_p    = ref_logp.exp()\n",
    "\n",
    "    mask = enc_c[\"attention_mask\"][:, :T_eff].to(ref_logp.dtype)       # [B, T_eff]\n",
    "\n",
    "    per_tok_kl = (ref_p * (ref_logp - tgt_logp)).sum(dim=-1)         # [B, T_eff]\n",
    "    kl_sum = (per_tok_kl * mask).sum(dim=-1)                            # [B]\n",
    "\n",
    "    tokenizer.padding_side = saved_pad_side\n",
    "    \n",
    "    del per_tok_kl, ref_logp, tgt_logp, ref_p, ref_logits, target_logits\n",
    "    return kl_sum.detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_bias(batch, ref_model, ref_tok, dpo_model, dpo_tok, n_responses_total=10):\n",
    "    \"\"\"\n",
    "    batch['prompt'] is a list of B prompts; we generate n_responses_total completions per prompt,\n",
    "    score them in batch on GPU, then average per-prompt to get one bias per prompt.\n",
    "    Returns: {'bias': [B-length list]}\n",
    "    \"\"\"\n",
    "    prompts = batch[\"prompt\"]                   # list of length B\n",
    "\n",
    "    # 1) Generate completions for all prompts in this batch (B * n rows)\n",
    "    new_ds = generate_responses(\n",
    "        ref_model, ref_tok, prompts,\n",
    "        max_new_tokens=512,\n",
    "        n_responses_total=n_responses_total,\n",
    "        batch_size=4,            # generation sub-batch (tune for VRAM)\n",
    "    )\n",
    "\n",
    "    # 2) KL(target||ref) on completions only (length = B*n)\n",
    "    kl_ref = get_expected_kl_batch(ref_model, ref_tok, dpo_model, new_ds)\n",
    "    kl_ref = torch.as_tensor(kl_ref, dtype=torch.float32)\n",
    "    # shape to [B, n] assuming generate_responses groups per prompt\n",
    "    B = len(prompts)\n",
    "    n = n_responses_total\n",
    "    kl_ref = kl_ref.view(B, n)\n",
    "    mean_kl_ref = (-0.1 * kl_ref.mean(dim=1))   # [B]\n",
    "    print (kl_ref)\n",
    "    print (mean_kl_ref)\n",
    "\n",
    "    # 3) Rewards r = 0.1 * (logp_dpo - logp_ref) for each (prompt, completion)\n",
    "    s_dpo = forward_batch(dpo_model, dpo_tok, new_ds, temperature=1.0)\n",
    "    s_ref = forward_batch(ref_model, ref_tok, new_ds, temperature=1.0)\n",
    "    s_dpo = torch.as_tensor(s_dpo, dtype=torch.float32)\n",
    "    s_ref = torch.as_tensor(s_ref, dtype=torch.float32)\n",
    "    r = 0.1 * (s_dpo - s_ref)                   # [B*n]\n",
    "    r = r.view(B, n)\n",
    "    mean_r_ref = r.mean(dim=1)                  # [B]\n",
    "    print (r)\n",
    "    print (mean_r_ref)\n",
    "\n",
    "    # 4) Bias per prompt\n",
    "    bias = (mean_kl_ref - mean_r_ref).tolist()  # list length B\n",
    "\n",
    "    return {\"bias\": bias}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b788b6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_instance, ref_tok = load_model(ref_policy_path, task = 'generation')\n",
    "dpo_instance, dpo_tok = load_model(dpo_policy_path, task = 'generation')\n",
    "ds = load_dataset(ds_path, cache_dir=data_cache_path, split = 'train')\n",
    "prompts = ds['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b2d906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[103.5000,  92.0000, 150.0000, 114.0000, 118.5000,  94.5000,  75.0000,\n",
      "          89.5000, 140.0000,  79.0000],\n",
      "        [ 55.7500,  55.5000, 102.5000,  89.0000,  76.0000, 104.5000,  91.5000,\n",
      "          80.0000, 101.0000, 145.0000],\n",
      "        [ 84.0000,  65.0000,  85.0000,  60.5000,  48.7500,  59.7500, 134.0000,\n",
      "          69.5000,  66.0000,  68.5000],\n",
      "        [106.0000, 105.5000, 101.0000,  60.0000, 139.0000,  99.5000,  79.0000,\n",
      "         103.0000, 142.0000,  70.0000]])\n",
      "tensor([-10.5600,  -9.0075,  -7.4100, -10.0500])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   4%|▍         | 4/100 [00:31<12:31,  7.83s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 28.5000,  31.5000,  33.7500,  71.0000,  33.0000,  26.8750,  54.7500,\n",
      "          28.5000,  32.0000, 122.5000],\n",
      "        [107.0000,  70.5000,  74.0000,  73.5000,  86.5000,  85.5000,  87.0000,\n",
      "         144.0000, 106.0000,  49.7500],\n",
      "        [ 38.5000,  36.7500,  59.7500,  38.0000,  32.0000,  39.5000,  29.5000,\n",
      "          39.7500,  32.0000,  39.5000],\n",
      "        [135.0000,  93.5000,  90.5000, 141.0000,  86.5000, 115.5000,  60.2500,\n",
      "         100.5000,  59.2500,  87.0000]])\n",
      "tensor([-4.6238, -8.8375, -3.8525, -9.6900])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   8%|▊         | 8/100 [00:49<08:58,  5.85s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 80.0000, 136.0000, 101.5000,  45.2500,  49.5000,  60.2500, 107.5000,\n",
      "         109.0000, 101.0000,  82.5000],\n",
      "        [ 86.5000, 115.0000,  77.0000, 168.0000, 119.0000, 130.0000,  53.0000,\n",
      "         109.5000,  68.0000,  74.0000],\n",
      "        [ 41.7500,  34.2500,  29.3750,  42.7500,  56.0000,  34.7500,  40.7500,\n",
      "          53.2500,  60.7500,  34.0000],\n",
      "        [ 74.0000,  59.7500, 130.0000,  82.5000,  72.0000, 114.0000,  59.5000,\n",
      "         103.0000,  85.0000, 105.0000]])\n",
      "tensor([ -8.7250, -10.0000,  -4.2763,  -8.8475])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  12%|█▏        | 12/100 [01:10<08:15,  5.63s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 49.5000,  52.0000,  28.7500,  55.0000,  56.7500,  53.0000,  56.7500,\n",
      "          81.0000,  53.7500,  45.0000],\n",
      "        [ 73.5000,  93.0000,  57.2500,  77.0000,  61.2500,  94.0000,  44.5000,\n",
      "          91.5000,  82.0000,  88.0000],\n",
      "        [ 35.0000, 126.5000,  47.2500, 137.0000,  55.5000,  96.5000, 130.0000,\n",
      "         169.0000, 109.0000, 115.5000],\n",
      "        [ 74.5000,  95.5000,  58.5000,  77.0000,  59.7500,  26.0000,  58.5000,\n",
      "          96.5000, 103.0000,  70.5000]])\n",
      "tensor([ -5.3150,  -7.6200, -10.2125,  -7.1975])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  16%|█▌        | 16/100 [01:41<08:59,  6.42s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 69.0000,  92.5000,  75.5000, 123.0000,  71.5000,  90.0000,  70.0000,\n",
      "          70.5000, 116.5000,  93.0000],\n",
      "        [ 49.5000,  88.0000,  40.7500,  36.7500,  79.0000,  59.2500,  69.5000,\n",
      "          76.5000,  78.0000,  23.0000],\n",
      "        [ 43.5000,  46.7500,  97.0000, 127.0000,  71.5000,  53.5000,  36.5000,\n",
      "          47.0000,  47.7500,  42.0000],\n",
      "        [ 60.5000, 121.5000,  98.5000, 119.0000,  83.0000, 160.0000, 149.0000,\n",
      "         135.0000,  99.5000,  70.5000]])\n",
      "tensor([ -8.7150,  -6.0025,  -6.1250, -10.9650])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  20%|██        | 20/100 [02:11<09:08,  6.86s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 70.5000,  94.5000,  91.5000,  87.5000, 112.0000,  71.5000, 105.0000,\n",
      "          70.0000,  80.5000,  68.5000],\n",
      "        [ 37.7500,  94.5000,  57.2500,  36.7500,  61.2500,  35.7500,  51.7500,\n",
      "          74.0000,  94.5000,  87.0000],\n",
      "        [116.0000,  77.5000, 116.0000,  63.7500,  75.5000, 125.5000, 108.0000,\n",
      "         167.0000,  66.0000,  55.5000],\n",
      "        [ 97.5000, 111.5000, 106.5000, 140.0000, 117.0000, 105.0000, 146.0000,\n",
      "          77.5000, 150.0000, 125.5000]])\n",
      "tensor([ -8.5150,  -6.3050,  -9.7075, -11.7650])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  24%|██▍       | 24/100 [02:40<08:48,  6.96s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 78.5000, 110.0000, 128.0000,  84.5000,  79.5000, 106.0000, 118.5000,\n",
      "         104.5000, 105.0000, 102.0000],\n",
      "        [ 21.6250,  22.3750,  29.5000,  13.7500,  22.0000,  20.2500,  22.3750,\n",
      "          22.3750,  13.8750,  12.6875],\n",
      "        [104.5000, 112.5000,  97.5000, 144.0000, 144.0000, 158.0000,  87.5000,\n",
      "         110.5000, 110.5000,  81.5000],\n",
      "        [ 93.0000,  90.0000,  78.5000,  64.0000,  85.5000, 110.0000,  28.5000,\n",
      "          72.5000,  42.0000,  69.5000]])\n",
      "tensor([-10.1650,  -2.0081, -11.5050,  -7.3350])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  28%|██▊       | 28/100 [03:03<07:52,  6.56s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 28.0000,  20.5000,  53.0000,  37.5000,  35.2500,  37.2500,  38.2500,\n",
      "          21.0000,  48.7500,  35.0000],\n",
      "        [ 92.5000, 102.0000,  74.5000,  67.0000,  99.0000,  56.2500,  65.0000,\n",
      "          79.0000,  73.0000,  65.0000],\n",
      "        [ 75.0000,  89.5000,  68.5000,  50.0000,  59.5000,  88.0000,  93.0000,\n",
      "          58.7500,  99.5000,  64.5000],\n",
      "        [103.5000,  66.5000,  42.2500,  64.0000,  75.5000,  78.0000,  36.0000,\n",
      "          57.0000,  63.0000,  45.2500]])\n",
      "tensor([-3.5450, -7.7325, -7.4625, -6.3100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  32%|███▏      | 32/100 [03:35<07:59,  7.05s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 30.3750,  56.5000,  77.5000,  48.0000,  57.7500,  45.7500,  52.7500,\n",
      "          69.5000,  38.2500, 100.5000],\n",
      "        [ 31.3750,  30.5000,  32.0000,  24.2500,  15.6250,  63.5000,  24.0000,\n",
      "          23.1250,  39.2500,  25.2500],\n",
      "        [ 75.5000,  90.0000,  58.5000,  70.5000,  67.0000,  84.5000,  71.5000,\n",
      "          67.5000,  51.2500,  99.5000],\n",
      "        [ 54.5000,  63.5000,  92.5000,  78.5000,  80.0000,  55.7500,  68.0000,\n",
      "          57.0000,  80.5000,  78.0000]])\n",
      "tensor([-5.7688, -3.0888, -7.3575, -7.0825])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  36%|███▌      | 36/100 [03:55<06:47,  6.36s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 78.5000,  76.0000,  61.5000, 116.0000, 100.5000,  84.5000,  81.0000,\n",
      "          71.5000,  71.5000,  87.5000],\n",
      "        [ 56.2500,  58.0000,  21.6250,  89.5000,  50.7500,  94.0000,  74.0000,\n",
      "          21.7500,  46.5000,  49.0000],\n",
      "        [132.0000,  94.5000,  57.5000,  65.5000,  98.0000,  57.5000,  83.0000,\n",
      "         140.0000,  89.0000, 132.0000],\n",
      "        [107.0000,  88.5000,  93.0000,  61.7500, 105.5000,  95.5000,  59.5000,\n",
      "          89.0000,  99.0000,  95.0000]])\n",
      "tensor([-8.2850, -5.6137, -9.4900, -8.9375])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  40%|████      | 40/100 [04:27<06:56,  6.94s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[121.5000, 165.0000, 123.0000, 123.5000, 140.0000, 115.0000, 114.0000,\n",
      "         125.5000, 152.0000, 103.5000],\n",
      "        [134.0000,  93.5000,  55.0000, 106.0000,  87.5000,  99.0000,  81.0000,\n",
      "         118.5000,  64.0000,  95.5000],\n",
      "        [111.0000,  94.5000, 132.0000, 181.0000,  66.5000,  88.0000,  70.0000,\n",
      "         172.0000,  79.5000, 104.5000],\n",
      "        [ 95.0000,  60.2500,  55.7500,  57.0000,  37.5000,  90.0000,  68.0000,\n",
      "          85.5000,  54.5000,  47.5000]])\n",
      "tensor([-12.8300,  -9.3400, -10.9900,  -6.5100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  44%|████▍     | 44/100 [04:57<06:35,  7.07s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.7500,  54.7500,  39.7500,  96.5000,  58.5000,  63.5000,  62.5000,\n",
      "          87.0000,  64.5000,  47.0000],\n",
      "        [106.0000, 121.5000, 137.0000, 124.0000,  68.5000, 150.0000, 120.5000,\n",
      "         111.5000,  96.5000,  85.5000],\n",
      "        [ 61.0000,  99.0000,  96.5000,  75.0000,  63.7500, 118.0000,  74.0000,\n",
      "          50.5000,  72.0000, 147.0000],\n",
      "        [ 31.3750,  43.0000,  85.5000,  64.0000,  44.2500,  79.5000,  40.0000,\n",
      "          58.2500,  84.5000,  35.5000]])\n",
      "tensor([ -6.3075, -11.2100,  -8.5675,  -5.6588])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  48%|████▊     | 48/100 [05:22<05:54,  6.81s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 98.0000,  65.5000,  39.0000, 126.5000, 105.0000,  86.0000,  52.5000,\n",
      "         124.0000,  42.7500, 118.0000],\n",
      "        [ 61.5000, 103.5000,  50.5000,  83.0000,  65.5000,  24.2500,  84.5000,\n",
      "          73.5000, 124.5000,  85.0000],\n",
      "        [ 35.0000, 178.0000,  53.0000, 112.0000,  24.6250, 106.0000,  70.5000,\n",
      "          96.0000, 243.0000,  59.7500],\n",
      "        [ 49.5000,  61.2500,  23.3750, 132.0000, 156.0000,  96.0000,  44.0000,\n",
      "         103.0000,  68.5000,  63.5000]])\n",
      "tensor([-8.5725, -7.5575, -9.7788, -7.9713])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  52%|█████▏    | 52/100 [05:50<05:30,  6.89s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 87.5000,  78.5000,  80.0000,  82.5000,  53.0000,  90.5000,  69.5000,\n",
      "          96.5000,  33.7500,  91.0000],\n",
      "        [ 78.0000,  37.2500,  34.2500, 103.5000,  46.7500,  56.5000,  77.5000,\n",
      "          74.0000,  49.7500,  65.0000],\n",
      "        [ 42.5000,  37.2500,  42.0000,  51.7500,  35.5000,  65.0000,  42.2500,\n",
      "          58.7500,  55.7500,  49.0000],\n",
      "        [ 36.5000,  29.7500, 153.0000,  79.0000, 129.0000,  82.5000,  35.2500,\n",
      "          36.0000,  63.0000,  35.7500]])\n",
      "tensor([-7.6275, -6.2250, -4.7975, -6.7975])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  56%|█████▌    | 56/100 [06:16<04:59,  6.80s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 84.0000,  96.0000, 139.0000,  52.7500,  81.0000, 127.0000, 153.0000,\n",
      "          89.5000, 107.0000, 124.5000],\n",
      "        [ 65.0000,  98.0000,  43.7500,  60.5000,  89.5000,  85.5000,  63.7500,\n",
      "         112.0000,  79.5000,  73.5000],\n",
      "        [ 99.5000, 139.0000, 126.0000,  47.7500,  85.5000,  63.2500,  71.5000,\n",
      "          86.0000,  66.5000,  48.0000],\n",
      "        [ 87.0000,  64.5000,  87.0000, 103.5000,  98.5000, 101.5000,  54.2500,\n",
      "         103.5000,  59.0000,  70.5000]])\n",
      "tensor([-10.5375,  -7.7100,  -8.3300,  -8.2925])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  60%|██████    | 60/100 [06:44<04:33,  6.85s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[147.0000, 161.0000,  82.0000, 114.0000, 115.5000,  65.5000, 142.0000,\n",
      "          47.2500,  76.5000, 168.0000],\n",
      "        [ 83.0000,  66.5000,  52.7500,  60.0000,  64.5000,  55.0000,  82.5000,\n",
      "          81.0000, 132.0000, 115.5000],\n",
      "        [ 69.0000,  87.5000,  58.2500,  91.0000,  75.5000, 141.0000,  65.5000,\n",
      "          57.2500,  85.0000,  84.5000],\n",
      "        [ 77.0000,  65.5000,  70.5000,  89.0000,  59.7500,  99.0000,  75.0000,\n",
      "          83.0000,  85.5000, 103.5000]])\n",
      "tensor([-11.1875,  -7.9275,  -8.1450,  -8.0775])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  64%|██████▍   | 64/100 [07:14<04:12,  7.01s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 89.5000, 125.5000,  66.0000, 119.0000, 106.0000,  91.0000,  65.5000,\n",
      "         107.5000,  93.0000,  59.0000],\n",
      "        [ 68.0000,  54.0000,  84.5000,  36.7500,  27.1250,  53.5000,  55.5000,\n",
      "          41.7500,  50.5000,  43.7500],\n",
      "        [ 44.5000,  87.0000, 102.0000,  49.2500,  39.5000,  91.5000,  68.0000,\n",
      "          49.2500,  83.0000,  48.2500],\n",
      "        [ 63.2500,  54.5000,  71.5000,  64.5000,  61.0000,  55.7500,  79.5000,\n",
      "          95.0000,  75.5000, 124.0000]])\n",
      "tensor([-9.2200, -5.1537, -6.6225, -7.4450])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  68%|██████▊   | 68/100 [07:36<03:29,  6.54s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[108.0000, 165.0000, 109.5000, 116.5000, 125.5000,  99.5000, 150.0000,\n",
      "         106.0000,  72.0000,  75.0000],\n",
      "        [ 74.5000, 188.0000,  83.0000,  65.0000, 131.0000,  71.0000,  55.5000,\n",
      "         198.0000,  88.5000,  39.2500],\n",
      "        [ 41.0000,  83.0000,  78.0000,  49.5000,  54.7500,  65.0000,  69.0000,\n",
      "          84.0000,  52.7500,  34.5000],\n",
      "        [ 91.5000,  57.5000,  95.5000,  61.7500, 174.0000, 114.5000, 104.0000,\n",
      "         149.0000,  67.0000,  57.0000]])\n",
      "tensor([-11.2700,  -9.9375,  -6.1150,  -9.7175])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  72%|███████▏  | 72/100 [08:05<03:08,  6.75s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 91.5000,  45.5000, 112.0000, 131.0000, 101.5000,  53.2500, 109.5000,\n",
      "          74.5000,  50.5000,  86.0000],\n",
      "        [ 92.0000,  71.5000,  55.2500, 108.0000,  86.5000,  89.0000, 140.0000,\n",
      "          56.5000,  54.2500,  58.2500],\n",
      "        [ 79.5000,  72.5000, 106.0000, 108.0000,  41.7500,  52.7500,  67.5000,\n",
      "          63.2500, 140.0000,  69.0000],\n",
      "        [103.0000,  73.0000,  89.5000,  64.0000,  56.2500,  50.5000, 126.5000,\n",
      "         132.0000, 112.5000,  60.2500]])\n",
      "tensor([-8.5525, -8.1125, -8.0025, -8.6750])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  76%|███████▌  | 76/100 [08:30<02:38,  6.60s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 84.5000,  66.0000,  54.0000,  43.5000, 104.5000,  98.0000,  98.0000,\n",
      "          41.7500,  77.5000, 103.0000],\n",
      "        [112.5000,  87.5000, 141.0000, 131.0000,  64.5000,  51.5000, 130.0000,\n",
      "         122.0000,  96.5000, 158.0000],\n",
      "        [274.0000,  85.5000,  78.0000,  76.5000,  68.5000,  51.0000,  45.5000,\n",
      "          62.7500,  43.7500,  47.7500],\n",
      "        [ 38.0000,   8.6250,   8.6250,  26.5000,  26.7500,   9.9375,  37.2500,\n",
      "          26.7500,   9.9375,  11.0625]])\n",
      "tensor([ -7.7075, -10.9450,  -8.3325,  -2.0344])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  80%|████████  | 80/100 [08:57<02:14,  6.71s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[141.0000, 123.5000, 120.5000, 131.0000, 132.0000, 113.5000, 117.5000,\n",
      "         160.0000, 126.0000, 114.5000],\n",
      "        [ 90.5000,  11.1250,  66.0000,  58.7500,  52.0000,  58.0000, 101.0000,\n",
      "          57.7500,  50.5000,  48.0000],\n",
      "        [ 38.5000,  34.5000,  39.5000,  39.5000,  36.7500,  40.2500,  40.2500,\n",
      "          27.6250,  39.5000,  36.7500],\n",
      "        [121.5000, 104.5000, 125.0000, 121.0000,  85.0000,  53.5000,  96.5000,\n",
      "          85.0000, 112.5000,  70.0000]])\n",
      "tensor([-12.7950,  -5.9363,  -3.7313,  -9.7450])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  84%|████████▍ | 84/100 [09:30<01:54,  7.15s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 57.5000,  55.2500,  72.5000, 107.5000, 105.0000,  73.0000, 135.0000,\n",
      "          68.0000,  42.7500,  45.2500],\n",
      "        [ 64.5000,  64.5000,  56.0000,  64.5000,  65.0000,  51.2500,  76.5000,\n",
      "          41.0000,  41.0000,  78.5000],\n",
      "        [ 20.1250,  25.5000,  20.1250,  25.5000,  25.5000,  27.0000,  22.1250,\n",
      "          31.8750,  30.0000,  31.8750],\n",
      "        [ 66.5000,  74.0000,  77.0000,  70.5000,  75.5000, 123.5000,  78.0000,\n",
      "          80.0000,  45.2500, 122.5000]])\n",
      "tensor([-7.6175, -6.0275, -2.5963, -8.1275])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  88%|████████▊ | 88/100 [09:58<01:25,  7.09s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  7.9688,   7.9688,   7.9688,  12.1250,   5.0625,   7.9688,   7.9688,\n",
      "          14.2500,  13.3750,  14.7500],\n",
      "        [ 57.0000,  79.5000,  84.0000,  49.2500,  74.0000, 118.5000,  47.0000,\n",
      "         139.0000,  98.5000,  51.2500],\n",
      "        [ 32.2500,  30.1250,  30.1250,  41.2500,  30.1250,  32.2500,  30.1250,\n",
      "          30.1250,  32.2500,  41.2500],\n",
      "        [101.0000,  67.0000,  50.0000,  87.5000,  70.0000,  95.5000,  60.2500,\n",
      "          52.7500,  48.5000, 159.0000]])\n",
      "tensor([-0.9941, -7.9800, -3.2987, -7.9150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  92%|█████████▏| 92/100 [10:17<00:51,  6.39s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 43.0000,  48.0000,  82.5000,  49.7500,  70.5000,  87.5000,  30.7500,\n",
      "          78.5000,  86.0000,  63.5000],\n",
      "        [ 66.0000,  86.5000,  80.5000,  58.5000, 133.0000,  58.5000,  55.2500,\n",
      "          93.5000,  71.5000,  68.0000],\n",
      "        [ 24.7500,  18.7500,  27.2500,  27.8750,  30.2500,  24.8750,  22.0000,\n",
      "          28.7500,  13.7500,  23.3750],\n",
      "        [108.0000, 149.0000, 113.5000, 108.0000, 116.5000, 114.0000,  84.0000,\n",
      "         118.5000,  93.5000,  72.0000]])\n",
      "tensor([ -6.4000,  -7.7125,  -2.4162, -10.7700])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  96%|█████████▌| 96/100 [10:38<00:24,  6.05s/ examples]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 77.5000,  70.0000,  98.0000, 132.0000,  76.5000,  70.0000,  55.7500,\n",
      "         127.5000, 149.0000,  47.5000],\n",
      "        [ 84.5000,  81.0000, 131.0000,  45.7500,  66.0000,  94.0000, 110.0000,\n",
      "          75.0000, 143.0000, 115.5000],\n",
      "        [ 99.0000,  76.0000,  92.5000,  80.0000,  67.0000,  53.7500, 122.0000,\n",
      "         106.5000,  43.2500, 150.0000],\n",
      "        [ 69.0000, 102.5000,  73.0000,  99.5000,  89.0000,  50.5000,  67.0000,\n",
      "         179.0000, 103.5000,  57.2500]])\n",
      "tensor([-9.0375, -9.4575, -8.9000, -8.9025])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [10:56<00:00,  6.57s/ examples]\n"
     ]
    }
   ],
   "source": [
    "test_ds = ds.select(range(100))\n",
    "a = test_ds.map(\n",
    "    calculate_bias,\n",
    "    batched = True, \n",
    "    batch_size = 4,\n",
    "    fn_kwargs = dict(\n",
    "        ref_model = ref_instance, \n",
    "        ref_tok = ref_tok, \n",
    "        dpo_model = dpo_instance, \n",
    "        dpo_tok = dpo_tok,\n",
    "        n_responses_total = 10\n",
    "    ),\n",
    "    load_from_cache_file = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "413e6fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': [{'content': 'How do I teach kids to meditate?', 'role': 'user'},\n",
       "  {'content': 'Great question! That’s a really useful skill to cultivate, it can bring peace, calm, and happiness. I’m glad you want to teach your kids about it.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'All right, so how do we start?', 'role': 'user'},\n",
       "  {'content': 'Well, we can get started with just being silent. You can tell the kids it’s okay if they just sit there quietly for a few minutes without thinking of anything.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'any other ideas? they are fidgeting', 'role': 'user'}],\n",
       " 'a1': [{'content': \"Sure! Another idea is to use guided imagery meditation. This involves visualizing a peaceful place or scene in your mind's eye. For example, imagine yourself on a beach with the sound of waves crashing gently in the background.\\n\\nAnother technique is mindfulness meditation, where you focus on your breath as it flows through your body. As you breathe, notice any thoughts that come up but don’t dwell on them; simply observe them and let go of them when you catch yourself focusing too much on them.\\n\\nIf your child is still fidgety during these sessions, try playing some soft music in the background or using a gentle tone or voice when guiding their attention inward. The key is to be patient and consistent, and remember that everyone learns at their own pace.\",\n",
       "   'role': 'assistant'}],\n",
       " 'a2': [{'content': \"Absolutely! Another way is to have them focus on their breathing, inhaling through one nostril at a time, then exhaling through another, or try counting each breath in their mind. It's like a simple game they can win as they breathe deeply. This method can be calming because it helps slow down racing thoughts.\",\n",
       "   'role': 'assistant'}],\n",
       " 'rank': 1,\n",
       " 'bias': -2.846874713897705}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec53773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ds = generate_responses(\n",
    "    ref_instance, ref_tok, p,\n",
    "    max_new_tokens=512,\n",
    "    n_responses_total=20,\n",
    "    batch_size=4,            # generation sub-batch (tune for VRAM)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f05fb5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-8.3750, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-0.1 * get_expected_kl_batch(ref_instance, ref_tok, dpo_instance, new_ds).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9869078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-5.6562, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1*(forward_batch(dpo_instance, dpo_tok, new_ds) - forward_batch(ref_instance, ref_tok, new_ds)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb74c344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column([-1.6599998474121094, -3.5874991416931152, -2.859999656677246, -3.2300004959106445, -1.7362501621246338])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcfe2ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 202.46ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (0 / 1)                :  14%|█▍        | 21.6kB /  155kB,  108kB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████|  155kB /  155kB,  259kB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████|  155kB /  155kB,  194kB/s  \n",
      "New Data Upload                         : 100%|██████████|  134kB /  134kB,  167kB/s  \n",
      "                                        : 100%|██████████|  155kB /  155kB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.38s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/august66/hh_qwen2.5_1.5b_with_bias_test_100/commit/5e0bcd79c4f9c9ea6e77df025fb4a198b9f021d4', commit_message='Upload dataset', commit_description='', oid='5e0bcd79c4f9c9ea6e77df025fb4a198b9f021d4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/august66/hh_qwen2.5_1.5b_with_bias_test_100', endpoint='https://huggingface.co', repo_type='dataset', repo_id='august66/hh_qwen2.5_1.5b_with_bias_test_100'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.push_to_hub('august66/hh_qwen2.5_1.5b_with_bias_test_100', split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f658fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 43835/43835 [00:00<00:00, 290013.61 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Prompt 1/5 ====\n",
      "tensor([[113.0000, 210.0000,  67.5000,  94.0000, 122.0000, 100.5000,  93.5000,\n",
      "          89.5000, 107.5000,  83.0000]])\n",
      "tensor([-10.8050])\n",
      "tensor([[-11.2000, -16.4000,  -7.2000,  -4.0000, -10.4000,  -6.4000,  -6.0000,\n",
      "          -5.8000,  -8.6000,  -7.8000]])\n",
      "tensor([-8.3800])\n",
      "bias (N=10): -2.425000\n",
      "MSE(r, mu_ref)             : 10.924215\n",
      "MSE(r, mu_ref - bias)      : 5.695763\n",
      "Relative improvement       : 47.86%\n",
      "\n",
      "==== Prompt 2/5 ====\n",
      "tensor([[135.0000, 121.0000,  85.5000, 116.0000, 129.0000, 107.0000, 134.0000,\n",
      "         110.0000, 145.0000,  90.5000]])\n",
      "tensor([-11.7300])\n",
      "tensor([[-12.8000, -10.8000,  -5.0000,  -4.6000, -10.2000, -10.4000, -11.4000,\n",
      "          -4.8000,  -7.6000,  -4.2000]])\n",
      "tensor([-8.1800])\n",
      "bias (N=10): -3.550000\n",
      "MSE(r, mu_ref)             : 15.560519\n",
      "MSE(r, mu_ref - bias)      : 5.289621\n",
      "Relative improvement       : 66.01%\n",
      "\n",
      "==== Prompt 3/5 ====\n",
      "tensor([[ 79.5000,  81.0000,  93.0000,  74.5000,  52.0000, 106.0000,  72.0000,\n",
      "          57.2500,  59.2500,  93.0000]])\n",
      "tensor([-7.6750])\n",
      "tensor([[-3.0000, -6.3000, -4.8000, -2.8000, -4.4000, -6.0000, -6.6000, -3.4000,\n",
      "         -5.8000, -3.5000]])\n",
      "tensor([-4.6600])\n",
      "bias (N=10): -3.015000\n",
      "MSE(r, mu_ref)             : 17.014641\n",
      "MSE(r, mu_ref - bias)      : 6.706749\n",
      "Relative improvement       : 60.58%\n",
      "\n",
      "==== Prompt 4/5 ====\n",
      "tensor([[ 73.0000,  71.5000, 127.5000, 109.5000, 102.0000, 141.0000, 139.0000,\n",
      "         104.0000, 108.0000,  99.0000]])\n",
      "tensor([-10.7450])\n",
      "tensor([[ -4.8000,  -8.0000,  -8.0000, -11.6000,  -4.2000,  -6.6000,  -2.8000,\n",
      "          -6.6000,  -9.6000,  -7.6000]])\n",
      "tensor([-6.9800])\n",
      "bias (N=10): -3.765000\n",
      "MSE(r, mu_ref)             : 14.571349\n",
      "MSE(r, mu_ref - bias)      : 7.398041\n",
      "Relative improvement       : 49.23%\n",
      "\n",
      "==== Prompt 5/5 ====\n",
      "tensor([[ 86.0000, 100.5000, 119.0000,  92.5000,  75.0000,  82.5000,  65.0000,\n",
      "         117.5000,  64.5000,  80.0000]])\n",
      "tensor([-8.8250])\n",
      "tensor([[-3.8250, -8.5250, -7.3500, -5.3250, -4.2000, -5.7500, -4.5500, -7.3375,\n",
      "         -7.6375, -7.4750]])\n",
      "tensor([-6.1975])\n",
      "bias (N=10): -2.627500\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# --- assume your helpers are already defined above in the notebook ---\n",
    "# load_model, generate_responses, forward_batch, get_expected_kl_batch, calculate_bias\n",
    "# ref_policy_path, dpo_policy_path, ds_path, model_cache_path\n",
    "\n",
    "# ---------------------- load models/tokenizers ----------------------\n",
    "ref_instance, ref_tok = load_model(ref_policy_path, task=\"generation\", model_type=\"decoder\", model_cache_path=model_cache_path)\n",
    "dpo_instance, dpo_tok = load_model(dpo_policy_path, task=\"generation\", model_type=\"decoder\", model_cache_path=model_cache_path)\n",
    "\n",
    "# ---------------------- load dataset & pick 5 prompts ----------------------\n",
    "def _first_available_split(dset_dict):\n",
    "    for key in [\"train\", \"validation\", \"test\"]:\n",
    "        if key in dset_dict:\n",
    "            return dset_dict[key]\n",
    "    # fall back to the first split\n",
    "    return dset_dict[list(dset_dict.keys())[0]]\n",
    "\n",
    "ds_all = load_dataset(ds_path)\n",
    "ds = _first_available_split(ds_all)\n",
    "prompts5 = ds[\"prompt\"][:5]  # works whether prompt is str or chat list\n",
    "\n",
    "# ---------------------- small batching wrappers to avoid OOM ----------------------\n",
    "def chunked_forward_scores(model, tok, pairs_ds: Dataset, chunk=16, temperature=1.0, device=\"cuda\"):\n",
    "    out = []\n",
    "    N = len(pairs_ds)\n",
    "    for i in range(0, N, chunk):\n",
    "        sub = pairs_ds[i : i + chunk]\n",
    "        sub_dict = {\"prompt\": sub[\"prompt\"], \"completion\": sub[\"completion\"]}\n",
    "        sc = forward_batch(model, tok, sub_dict, temperature=temperature, device=device)  # CPU tensor\n",
    "        out.append(sc)\n",
    "        del sc\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    return torch.cat(out) if len(out) > 1 else out[0]\n",
    "\n",
    "def chunked_expected_kl(ref_model, tok, tgt_model, pairs_ds: Dataset, chunk=16, device=\"cuda\"):\n",
    "    out = []\n",
    "    N = len(pairs_ds)\n",
    "    for i in range(0, N, chunk):\n",
    "        sub = pairs_ds[i : i + chunk]\n",
    "        sub_dict = {\"prompt\": sub[\"prompt\"], \"completion\": sub[\"completion\"]}\n",
    "        kl = get_expected_kl_batch(ref_model, tok, tgt_model, sub_dict, device=device)  # CPU tensor\n",
    "        out.append(kl)\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    return torch.cat(out) if len(out) > 1 else out[0]\n",
    "\n",
    "def mse(a: torch.Tensor, b: torch.Tensor) -> float:\n",
    "    return float(((a - b) ** 2).mean().item())\n",
    "\n",
    "# ---------------------- main loop ----------------------\n",
    "TEMPERATURE_BIAS = 1.0         # used inside your helper defaults\n",
    "TEMPERATURE_EVAL = 1.0         # for scoring\n",
    "MAX_NEW_TOKENS = 512\n",
    "N_BIAS = 10\n",
    "N_EVAL = 500\n",
    "CHUNK = 16                     # safe GPU chunk size\n",
    "SCALE = 0.1                    # your 0.1 factor for r and mu_ref\n",
    "\n",
    "results = []\n",
    "\n",
    "for pi, p in enumerate(prompts5, start=1):\n",
    "    print(f\"\\n==== Prompt {pi}/5 ====\")\n",
    "\n",
    "    # --- 1) estimate bias with 10 samples (from ref) ---\n",
    "    bias_dict = calculate_bias({\"prompt\": [p]}, ref_instance, ref_tok, dpo_instance, dpo_tok, n_responses_total=N_BIAS)\n",
    "    bias = float(bias_dict[\"bias\"][0])\n",
    "    print(f\"bias (N={N_BIAS}): {bias:.6f}\")\n",
    "\n",
    "    # --- 2) sample another 500 responses (from ref) for evaluation ---\n",
    "    eval_pairs = generate_responses(\n",
    "        ref_instance, ref_tok, [p],\n",
    "        temperature=TEMPERATURE_BIAS, max_new_tokens=MAX_NEW_TOKENS,\n",
    "        n_responses_total=N_EVAL, responses_per_call=16, batch_size=4,\n",
    "    )  # Dataset with columns: prompt, completion\n",
    "\n",
    "    # --- 3) compute per-sample mu_ref = -0.1 * KL(ref || dpo) ---\n",
    "    kl_vec = chunked_expected_kl(ref_instance, ref_tok, dpo_instance, eval_pairs, chunk=CHUNK, device=\"cuda\")  # tensor [N_EVAL]\n",
    "    mu_ref = (-SCALE * kl_vec).to(torch.float32)  # [N_EVAL]\n",
    "\n",
    "    # --- 4) compute per-sample r = 0.1 * (logp_dpo - logp_ref) ---\n",
    "    s_dpo = chunked_forward_scores(dpo_instance, dpo_tok, eval_pairs, chunk=CHUNK, temperature=TEMPERATURE_EVAL, device=\"cuda\")  # [N_EVAL]\n",
    "    s_ref = chunked_forward_scores(ref_instance, ref_tok, eval_pairs, chunk=CHUNK, temperature=TEMPERATURE_EVAL, device=\"cuda\")  # [N_EVAL]\n",
    "    r = (SCALE * (s_dpo - s_ref)).to(torch.float32)  # [N_EVAL]\n",
    "\n",
    "    # --- 5) MSEs: raw vs. bias-corrected ---\n",
    "    mse_raw = mse(r, mu_ref)\n",
    "    mu_ref_bias_corrected = mu_ref - bias\n",
    "    mse_corrected = mse(r, mu_ref_bias_corrected)\n",
    "    improvement = (mse_raw - mse_corrected) / max(1e-12, mse_raw)\n",
    "\n",
    "    print(f\"MSE(r, mu_ref)             : {mse_raw:.6f}\")\n",
    "    print(f\"MSE(r, mu_ref - bias)      : {mse_corrected:.6f}\")\n",
    "    print(f\"Relative improvement       : {100.0*improvement:.2f}%\")\n",
    "\n",
    "    results.append({\n",
    "        \"prompt_index\": pi-1,\n",
    "        \"bias\": bias,\n",
    "        \"mse_raw\": mse_raw,\n",
    "        \"mse_corrected\": mse_corrected,\n",
    "        \"improvement\": improvement,\n",
    "    })\n",
    "\n",
    "# Optional: results as a small table\n",
    "try:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(results)\n",
    "    print(\"\\nSummary:\")\n",
    "    print(df.to_string(index=False))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6e256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
