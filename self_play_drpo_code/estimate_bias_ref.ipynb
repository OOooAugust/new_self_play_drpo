{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2195eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys, pathlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification,DebertaV2ForSequenceClassification, GPTNeoXForCausalLM\n",
    "from llm_blender.pair_ranker.pairrm import DebertaV2PairRM\n",
    "from transformers import DataCollatorWithPadding\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "LOCAL_TRL_PARENT = \"/workspace/Self_play_DRPO\"\n",
    "if LOCAL_TRL_PARENT not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_TRL_PARENT)\n",
    "\n",
    "    \n",
    "# now the import will use your local copy:\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    ModelConfig,\n",
    "    DRPOTrainer,\n",
    "    DRPOConfig,\n",
    ")\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "from trl.trainer.utils import pad, truncate_right, selective_log_softmax\n",
    "from trl.data_utils import apply_chat_template\n",
    "\n",
    "data_cache_path = \"/workspace/dataset\"\n",
    "model_cache_path = '/workspace/model_cache'\n",
    "ref_policy_path = \"Qwen/Qwen2.5-1.5B-Instruct\" \n",
    "#target_policy_path = \"Qwen/Qwen2.5-1.5B-Instruct\" \n",
    "reward_model_path = 'Kyleyee/Qwen2.5-1.5B-reward-hh-retrain'\n",
    "ds_path = 'august66/hh_qwen2.5_1.5b_with_bias'\n",
    "\n",
    "\n",
    "def load_model(model_path, task = 'generation', model_type = 'decoder', model_cache_path =  '/workspace/model_cache'):\n",
    "\n",
    "    model_args = ModelConfig(model_path)\n",
    "    #model_torch_dtype = (model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype))\n",
    "    model_torch_dtype = torch.bfloat16 \n",
    "    model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "    padding_side = 'left' if model_type == 'decoder' else 'right'\n",
    "    truncation_side = 'left' if model_type == 'decoder' else 'right'\n",
    "\n",
    "    if task == 'generation':\n",
    "        model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "\n",
    "    elif task == 'reward':\n",
    "        model_instance = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "    \n",
    "\n",
    "    model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, \n",
    "        padding_side = padding_side, \n",
    "        truncation_side = truncation_side,\n",
    "        use_fast = True,\n",
    "        trust_remote_code = model_args.trust_remote_code,\n",
    "        cache_dir = model_cache_path\n",
    "    )\n",
    "\n",
    "    if model_tokenizer.pad_token is None:\n",
    "        model_tokenizer.pad_token = model_tokenizer.eos_token\n",
    "\n",
    "    if getattr(model_instance.config, \"pad_token_id\", None) is None:\n",
    "        model_instance.config.pad_token_id = model_tokenizer.pad_token_id\n",
    "\n",
    "    if model_tokenizer.eos_token is None:\n",
    "        model_tokenizer.eos_token = model_tokenizer.pad_token  \n",
    "\n",
    "    if getattr(model_instance.config, \"eos_token_id\", None) is None:\n",
    "        model_instance.config.eos_token_id = model_tokenizer.eos_token_id\n",
    "\n",
    "    return model_instance, model_tokenizer\n",
    "\n",
    "\n",
    "def generate_responses(\n",
    "    model_instance,\n",
    "    model_tokenizer,\n",
    "    prompts,                      # single chat or list of chats (each: list[{role,content}])\n",
    "    *,\n",
    "    temperature=1.0,\n",
    "    max_new_tokens=256,\n",
    "    n_responses_total=1000,       # total per prompt\n",
    "    responses_per_call=16,        # per subcall per prompt\n",
    "    batch_size=4,                 # prompts per batch\n",
    "    device='cuda',\n",
    "    max_length=1024,\n",
    "):\n",
    "    # tokenizer prefs\n",
    "    model_tokenizer.padding_side = \"left\"\n",
    "    if hasattr(model_tokenizer, \"truncation_side\"):\n",
    "        model_tokenizer.truncation_side = \"left\"\n",
    "    if model_tokenizer.pad_token_id is None and model_tokenizer.eos_token_id is not None:\n",
    "        model_tokenizer.pad_token_id = model_tokenizer.eos_token_id\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"do_sample\": bool(temperature > 0),\n",
    "        \"temperature\": float(temperature),\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"eos_token_id\": model_tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": model_tokenizer.pad_token_id,\n",
    "        \"use_cache\": True,\n",
    "        \"return_dict_in_generate\": False,\n",
    "        \"output_scores\": False,\n",
    "        \"output_attentions\": False,\n",
    "        \"output_hidden_states\": False,\n",
    "    }\n",
    "\n",
    "    # normalize prompts input\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "    elif (isinstance(prompts, list) and prompts\n",
    "          and isinstance(prompts[0], dict) and \"role\" in prompts[0]):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    model_instance.to(device).eval()\n",
    "\n",
    "    prompts_out, completions_out = [], []\n",
    "\n",
    "    for s in range(0, len(prompts), batch_size):\n",
    "        batch_prompts_raw = prompts[s:s+batch_size]\n",
    "\n",
    "        # render once per batch\n",
    "        batch_rendered = [\n",
    "            model_tokenizer.apply_chat_template(p, add_generation_prompt=True, tokenize=False)\n",
    "            for p in batch_prompts_raw\n",
    "        ]\n",
    "        enc = model_tokenizer(\n",
    "            batch_rendered, padding=True, truncation=True,\n",
    "            max_length=max_length, return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        T_in = enc[\"input_ids\"].size(1)\n",
    "\n",
    "        # bucket completions per prompt index within this batch\n",
    "        per_prompt_bucket = [[] for _ in range(len(batch_prompts_raw))]\n",
    "\n",
    "        remaining = n_responses_total\n",
    "        while remaining > 0:\n",
    "            cur_n = min(responses_per_call, remaining)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_ids = model_instance.generate(\n",
    "                    **enc, num_return_sequences=cur_n, **gen_kwargs\n",
    "                )  # [B*cur_n, T_in + gen_len]\n",
    "\n",
    "            comp_ids = out_ids[:, T_in:]\n",
    "            decoded = model_tokenizer.batch_decode(comp_ids, skip_special_tokens=True)\n",
    "\n",
    "            # outputs are stacked: p0 x cur_n, p1 x cur_n, ...\n",
    "            total = comp_ids.size(0)  # B*cur_n\n",
    "            for k in range(total):\n",
    "                base_i = k // cur_n  # which prompt in this subcall\n",
    "                per_prompt_bucket[base_i].append(decoded[k])\n",
    "\n",
    "            # free between subcalls\n",
    "            del out_ids, comp_ids, decoded\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            remaining -= cur_n\n",
    "\n",
    "        # now FLATTEN in prompt order so each prompt's 1..N completions are contiguous\n",
    "        for i, completions in enumerate(per_prompt_bucket):\n",
    "            for c in completions:\n",
    "                prompts_out.append(batch_prompts_raw[i])  # original chat object\n",
    "                completions_out.append(c)\n",
    "\n",
    "        del enc\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return Dataset.from_dict({\"prompt\": prompts_out, \"completion\": completions_out})\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_rewards(\n",
    "    prompts,\n",
    "    a1s,\n",
    "    a2s,\n",
    "    reward_tok,\n",
    "    reward_model,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (reward_a1, reward_a2) for each (prompt, a1/a2) pair.\n",
    "    prompts/a1s/a2s can be chat-format (list of {\"role\",\"content\"}) or strings.\n",
    "    Chat rendering is done inline (no extra helper functions).\n",
    "    \"\"\"\n",
    "    device = 'cuda'\n",
    "    reward_model.to(device)\n",
    "    n = len(prompts)\n",
    "    texts = []\n",
    "\n",
    "    for p, a in zip(prompts, a1s):\n",
    "        # --- render (prompt + a1) ---\n",
    "        if isinstance(p, list) and p and isinstance(p[0], dict) and \"role\" in p[0]:\n",
    "            # prompt is chat\n",
    "            if isinstance(a, list) and a and isinstance(a[0], dict) and \"role\" in a[0]:\n",
    "                chat = p + a\n",
    "            else:\n",
    "                chat = p + [{\"role\": \"assistant\", \"content\": str(a)}]\n",
    "            text = reward_tok.apply_chat_template(\n",
    "                chat, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "        else:\n",
    "            # fallback: plain strings\n",
    "            text = f\"{str(p)}\\n{str(a)}\"\n",
    "        texts.append(text)\n",
    "\n",
    "    for p, a in zip(prompts, a2s):\n",
    "        # --- render (prompt + a2) ---\n",
    "        if isinstance(p, list) and p and isinstance(p[0], dict) and \"role\" in p[0]:\n",
    "            if isinstance(a, list) and a and isinstance(a[0], dict) and \"role\" in a[0]:\n",
    "                chat = p + a\n",
    "            else:\n",
    "                chat = p + [{\"role\": \"assistant\", \"content\": str(a)}]\n",
    "            text = reward_tok.apply_chat_template(\n",
    "                chat, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "        else:\n",
    "            text = f\"{str(p)}\\n{str(a)}\"\n",
    "        texts.append(text)\n",
    "\n",
    "    inputs = reward_tok(\n",
    "        texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    "    ).to(device)\n",
    "\n",
    "    logits = reward_model(**inputs).logits  # [2n, 1] or [2n, K] or [2n]\n",
    "    if logits.ndim == 2 and logits.size(-1) == 1:\n",
    "        scores = logits.squeeze(-1)                  # [2n]\n",
    "    elif logits.ndim == 2 and logits.size(-1) > 1:\n",
    "        scores = logits[:, -1]                       # common: last logit as reward\n",
    "    else:\n",
    "        scores = logits\n",
    "    scores = scores.detach().cpu().tolist()\n",
    "\n",
    "    return scores[:n], scores[n:]  # reward_a1, reward_a2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def forward_batch(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    batch,                         # dict with batch['prompt'], batch['completion']\n",
    "    *,\n",
    "    temperature=1.0,\n",
    "    max_length=1024,\n",
    "    add_generation_prompt=True,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Single batched pass. Returns tensor [B] with sum log-prob of each completion\n",
    "    conditioned on its prompt.\n",
    "    \"\"\"\n",
    "    prompts      = batch[\"prompt\"]\n",
    "    completions  = [str(c) for c in batch[\"completion\"]]\n",
    "    if len(completions) == 0:\n",
    "        return torch.empty(0)\n",
    "\n",
    "    # Render chat prompts where needed\n",
    "    def render(p):\n",
    "        if isinstance(p, list) and p and isinstance(p[0], dict) and \"role\" in p[0]:\n",
    "            return tokenizer.apply_chat_template(p, add_generation_prompt=add_generation_prompt, tokenize=False)\n",
    "        return str(p)\n",
    "    prompts_text = [render(p) for p in prompts]\n",
    "\n",
    "    # Temporarily force LEFT padding so (T_p_max - 1) is a real token\n",
    "    saved_pad_side = getattr(tokenizer, \"padding_side\", \"right\")\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    # Tokenize prompt & completion (batched)\n",
    "    enc_p = tokenizer(prompts_text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "    enc_c = tokenizer(completions,  padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Concat (teacher forcing)\n",
    "    input_ids = torch.cat([enc_p[\"input_ids\"], enc_c[\"input_ids\"]], dim=1)\n",
    "    attn_mask = torch.cat([enc_p[\"attention_mask\"], enc_c[\"attention_mask\"]], dim=1)\n",
    "\n",
    "    # Forward once\n",
    "    logits = model(input_ids=input_ids, attention_mask=attn_mask).logits  # [B, T, V]\n",
    "    if temperature and temperature > 0:\n",
    "        logits = logits / (temperature + 1e-7)\n",
    "\n",
    "    # Align to completion window (next-token prediction)\n",
    "    T_p = enc_p[\"input_ids\"].size(1)\n",
    "    T_c = enc_c[\"input_ids\"].size(1)\n",
    "    start = T_p - 1\n",
    "    end   = min(start + T_c, logits.size(1))\n",
    "    T_eff = end - start\n",
    "\n",
    "    win_logits = logits[:, start:end, :]                         # [B, T_eff, V]\n",
    "    tgt_ids = enc_c[\"input_ids\"][:, :T_eff]                      # [B, T_eff]\n",
    "    tgt_msk = enc_c[\"attention_mask\"][:, :T_eff].to(torch.bool)  # [B, T_eff]\n",
    "\n",
    "    # Your original scoring helper\n",
    "    logps  = selective_log_softmax(win_logits, tgt_ids)          # [B, T_eff]\n",
    "    scores = (logps * tgt_msk).sum(dim=1)                        # [B]\n",
    "\n",
    "    # Restore tokenizer padding side\n",
    "    tokenizer.padding_side = saved_pad_side\n",
    "    \n",
    "    del win_logits, tgt_ids, tgt_msk, logits\n",
    "    return scores.detach().cpu()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_expected_kl_batch(\n",
    "    ref_model,\n",
    "    tokenizer,                     # same tokenizer for both models\n",
    "    target_model,\n",
    "    batch,                         # dict with batch['prompt'], batch['completion']\n",
    "    *,\n",
    "    max_length=1024,\n",
    "    add_generation_prompt=True,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Single batched pass. Returns python list [B] with KL per example over completion tokens.\n",
    "    \"\"\"\n",
    "    prompts     = batch[\"prompt\"]\n",
    "    completions = [str(c) for c in batch[\"completion\"]]\n",
    "    if len(completions) == 0:\n",
    "        return []\n",
    "\n",
    "    # Render prompts\n",
    "    def render(p):\n",
    "        if isinstance(p, list) and p and isinstance(p[0], dict) and \"role\" in p[0]:\n",
    "            return tokenizer.apply_chat_template(p, add_generation_prompt=add_generation_prompt, tokenize=False)\n",
    "        return str(p)\n",
    "    prompts_text = [render(p) for p in prompts]\n",
    "\n",
    "    # LEFT padding\n",
    "    saved_pad_side = getattr(tokenizer, \"padding_side\", \"right\")\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    ref_model    = ref_model.to(device).eval()\n",
    "    target_model = target_model.to(device).eval()\n",
    "\n",
    "    # Tokenize\n",
    "    enc_p = tokenizer(prompts_text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "    enc_c = tokenizer(completions,  padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Concat\n",
    "    input_ids = torch.cat([enc_p[\"input_ids\"], enc_c[\"input_ids\"]], dim=1)\n",
    "    attn_mask = torch.cat([enc_p[\"attention_mask\"], enc_c[\"attention_mask\"]], dim=1)\n",
    "\n",
    "    # Forwards (batched)\n",
    "    ref_logits    = ref_model(input_ids=input_ids,    attention_mask=attn_mask).logits\n",
    "    target_logits = target_model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
    "\n",
    "    # Slice to completion region (next-token prediction)\n",
    "    T_p = enc_p[\"input_ids\"].size(1)\n",
    "    T_c = enc_c[\"input_ids\"].size(1)\n",
    "    start = T_p - 1\n",
    "    end   = min(start + T_c, target_logits.size(1))\n",
    "    T_eff = end - start\n",
    "\n",
    "    ref_logp = F.log_softmax(ref_logits[:,    start:end, :], dim=-1)   # [B, T_eff, V]\n",
    "    tgt_logp = F.log_softmax(target_logits[:, start:end, :], dim=-1)   # [B, T_eff, V]\n",
    "    ref_p    = ref_logp.exp()\n",
    "\n",
    "    mask = enc_c[\"attention_mask\"][:, :T_eff].to(ref_logp.dtype)       # [B, T_eff]\n",
    "\n",
    "    per_tok_kl = (ref_p * (ref_logp - tgt_logp)).sum(dim=-1)         # [B, T_eff]\n",
    "    kl_sum = (per_tok_kl * mask).sum(dim=-1)                            # [B]\n",
    "\n",
    "    tokenizer.padding_side = saved_pad_side\n",
    "    \n",
    "    del per_tok_kl, ref_logp, tgt_logp, ref_p, ref_logits, target_logits\n",
    "    return kl_sum.detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "def calculate_bias(batch, ref_model, ref_tok, dpo_model, dpo_tok, n_responses_total=10):\n",
    "    \"\"\"\n",
    "    batch['prompt'] is a list of B prompts; we generate n_responses_total completions per prompt,\n",
    "    score them in batch on GPU, then average per-prompt to get one bias per prompt.\n",
    "    Returns: {'bias': [B-length list]}\n",
    "    \"\"\"\n",
    "    prompts = batch[\"prompt\"]                   # list of length B\n",
    "\n",
    "    # 1) Generate completions for all prompts in this batch (B * n rows)\n",
    "    new_ds = generate_responses(\n",
    "        ref_model, ref_tok, prompts,\n",
    "        max_new_tokens=512,\n",
    "        n_responses_total=n_responses_total,\n",
    "        batch_size=4,            # generation sub-batch (tune for VRAM)\n",
    "    )\n",
    "\n",
    "    # 2) KL(target||ref) on completions only (length = B*n)\n",
    "    kl_ref = get_expected_kl_batch(ref_model, ref_tok, dpo_model, new_ds)\n",
    "    kl_ref = torch.as_tensor(kl_ref, dtype=torch.float32)\n",
    "    # shape to [B, n] assuming generate_responses groups per prompt\n",
    "    B = len(prompts)\n",
    "    n = n_responses_total\n",
    "    kl_ref = kl_ref.view(B, n)\n",
    "    mean_kl_ref = (-0.1 * kl_ref.mean(dim=1))   # [B]\n",
    "    print (kl_ref)\n",
    "    print (mean_kl_ref)\n",
    "\n",
    "    # 3) Rewards r = 0.1 * (logp_dpo - logp_ref) for each (prompt, completion)\n",
    "    s_dpo = forward_batch(dpo_model, dpo_tok, new_ds, temperature=1.0)\n",
    "    s_ref = forward_batch(ref_model, ref_tok, new_ds, temperature=1.0)\n",
    "    s_dpo = torch.as_tensor(s_dpo, dtype=torch.float32)\n",
    "    s_ref = torch.as_tensor(s_ref, dtype=torch.float32)\n",
    "    r = 0.1 * (s_dpo - s_ref)                   # [B*n]\n",
    "    r = r.view(B, n)\n",
    "    mean_r_ref = r.mean(dim=1)                  # [B]\n",
    "    print (r)\n",
    "    print (mean_r_ref)\n",
    "\n",
    "    # 4) Bias per prompt\n",
    "    bias = (mean_kl_ref - mean_r_ref).tolist()  # list length B\n",
    "\n",
    "    return {\"bias\": bias}\n",
    "\n",
    "import math\n",
    "def map_rank_and_reassign_chat(\n",
    "    batch,\n",
    "    reward_tok,\n",
    "    reward_model\n",
    "):\n",
    "    \"\"\"\n",
    "    Single function for ds.map:\n",
    "      - computes reward_a1/reward_a2 via compute_rewards(...)\n",
    "      - BT preference\n",
    "      - reassign preferred -> a1, dispreferred -> a2\n",
    "    \"\"\"\n",
    "    prompts = batch[\"prompt\"]\n",
    "    a1s = batch[\"a1\"]\n",
    "    a2s = batch[\"a2\"]\n",
    "\n",
    "    reward_a1, reward_a2 = compute_rewards(\n",
    "        prompts, a1s, a2s,\n",
    "        reward_tok=reward_tok,\n",
    "        reward_model=reward_model\n",
    "    )\n",
    "\n",
    "    bt_prob_a1_pref = [1.0 / (1.0 + math.exp(-(s1 - s2)))\n",
    "                       for s1, s2 in zip(reward_a1, reward_a2)]\n",
    "\n",
    "    new_a1, new_a2, swapped, pref_label, diff = [], [], [], [], []\n",
    "    for a1, a2, s1, s2 in zip(a1s, a2s, reward_a1, reward_a2):\n",
    "        d = s1 - s2\n",
    "        diff.append(d)\n",
    "        if d > 0:\n",
    "            new_a1.append(a1); new_a2.append(a2)\n",
    "            swapped.append(False); pref_label.append(\"a1\")\n",
    "        elif d < 0:\n",
    "            new_a1.append(a2); new_a2.append(a1)\n",
    "            swapped.append(True);  pref_label.append(\"a2\")\n",
    "        else:\n",
    "            new_a1.append(a1); new_a2.append(a2)\n",
    "            swapped.append(False); pref_label.append(\"tie\")\n",
    "\n",
    "    out = {\n",
    "        \"a1\": new_a1,\n",
    "        \"a2\": new_a2,\n",
    "        \"bt_prob_a1_pref\": bt_prob_a1_pref\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21e40afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "reward_model, reward_model_instance = load_model(reward_model_path, task = 'reward')\n",
    "ds_dict = load_dataset(ds_path)\n",
    "ds = concatenate_datasets(list(ds_dict.values()))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f7479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 18000/18000 [08:30<00:00, 35.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_bt_pref = ds.map(map_rank_and_reassign_chat,\n",
    "                  fn_kwargs = {'reward_tok':reward_model_instance, 'reward_model':reward_model}, batched = True, batch_size = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e3e44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 18/18 [00:00<00:00, 76.64ba/s]\n",
      "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            \n",
      "\u001b[A\n",
      "Processing Files (0 / 1)                :  14%|█▍        | 3.83MB / 26.7MB, 9.57MB/s  \n",
      "Processing Files (0 / 1)                :  22%|██▏       | 5.93MB / 26.7MB, 9.87MB/s  \n",
      "Processing Files (0 / 1)                :  38%|███▊      | 10.1MB / 26.7MB, 12.7MB/s  \n",
      "Processing Files (0 / 1)                :  67%|██████▋   | 18.0MB / 26.7MB, 18.0MB/s  \n",
      "Processing Files (0 / 1)                :  99%|█████████▉| 26.4MB / 26.7MB, 22.0MB/s  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 26.7MB / 26.7MB, 14.8MB/s  \n",
      "\u001b[A\n",
      "Processing Files (1 / 1)                : 100%|██████████| 26.7MB / 26.7MB, 13.3MB/s  \n",
      "New Data Upload                         : 100%|██████████| 23.4MB / 23.4MB, 11.7MB/s  \n",
      "                                        : 100%|██████████| 26.7MB / 26.7MB            \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.20s/ shards]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/august66/hh_qwen2.5_1.5b_with_bias_bt_pref/commit/7cae78f267bc96bfa601b6d7bf092344a54d55dc', commit_message='Upload dataset', commit_description='', oid='7cae78f267bc96bfa601b6d7bf092344a54d55dc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/august66/hh_qwen2.5_1.5b_with_bias_bt_pref', endpoint='https://huggingface.co', repo_type='dataset', repo_id='august66/hh_qwen2.5_1.5b_with_bias_bt_pref'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_bt_pref.push_to_hub('august66/hh_qwen2.5_1.5b_with_bias_bt_pref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c2628e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
