{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817259b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n",
      "/workspace/Self_play_DRPO/envs/my_env/lib/python3.12/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type load_checkpoint detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/workspace/Self_play_DRPO/envs/my_env/lib/python3.12/site-packages/dataclasses_json/core.py:201: RuntimeWarning: 'NoneType' object value of non-optional type device detected when decoding RankerConfig.\n",
      "  warnings.warn(\n",
      "/workspace/Self_play_DRPO/envs/my_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /root/.cache/huggingface/hub/llm-blender/PairRM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys, pathlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "LOCAL_TRL_PARENT = \"/workspace/Self_play_DRPO\"\n",
    "if LOCAL_TRL_PARENT not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_TRL_PARENT)\n",
    "import llm_blender\n",
    "blender = llm_blender.Blender()\n",
    "blender.loadranker(\"llm-blender/PairRM\") \n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    ModelConfig,\n",
    "    DRPOTrainer,\n",
    "    DRPOConfig,\n",
    ")\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "from trl.data_utils import apply_chat_template\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "data_cache_path = \"/workspace/dataset\"\n",
    "ultrafeedback_ds = load_dataset('august66/DRPO_data_from_ultrafeed_new_template', split=\"train\", cache_dir=data_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ultrafeedback_ds['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387ebd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "cache_path = \"/workspace/model_cache\"\n",
    "\n",
    "model_args = ModelConfig(model_name)\n",
    "model_torch_dtype = (model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype))\n",
    "\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_path,\n",
    "    torch_dtype=model_torch_dtype,\n",
    "    trust_remote_code=True,\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"left\",\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=cache_path,\n",
    ")\n",
    "\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = SIMPLE_CHAT_TEMPLATE\n",
    "\n",
    "\n",
    "def generate_text(prompts, tokenizer, model, temperature):\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    generate_kwargs = {\n",
    "        \"max_new_tokens\": 2048,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"do_sample\": temperature > 0,\n",
    "        \"num_return_sequences\": 2\n",
    "    }\n",
    "    \n",
    "    if temperature > 0:\n",
    "        generate_kwargs[\"temperature\"] = temperature\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        **generate_kwargs\n",
    "    )\n",
    "    \n",
    "    generated_ids = outputs[:, inputs.input_ids.shape[1]:]\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "def get_preference(prompts, response_1, response_2):\n",
    "    compare_result = blender.compare(prompts, response_1, response_2)\n",
    "    if compare_result[0]:\n",
    "        a1 =response_1\n",
    "        a2 = response_2\n",
    "    else:\n",
    "        a2 = response_1\n",
    "        a1 = response_2\n",
    "    return a1, a2\n",
    "\n",
    "def truncate_human(texts):\n",
    "    return [text.split(\"\\n\\nHuman\")[0] for text in texts]\n",
    "\n",
    "def extract_dialogue(examples: dict, tokenizer, model, temperature: float) -> dict:\n",
    "    prompts = examples[\"prompt\"]\n",
    "    chat_prompts = [apply_chat_template({\"prompt\": p}, tokenizer) for p in prompts]\n",
    "    flat_prompts = [x[\"prompt\"] for x in chat_prompts]\n",
    "    responses = generate_text(flat_prompts, tokenizer, model, temperature)\n",
    "    responses = truncate_human(responses)\n",
    "    return {\n",
    "        \"generated_response\": responses\n",
    "    }\n",
    "\n",
    "def prepare_dataset(batch, tokenizer = tokenizer, model = lm_model, temperature = 1.0):\n",
    "    responses = extract_dialogue(batch, tokenizer, model, temperature)['generated_response']\n",
    "    a1_list = []\n",
    "    a2_list = []\n",
    "    n = len(responses)//2\n",
    "    prompts = [p[0][\"content\"] for p in batch[\"prompt\"]]\n",
    "    for i in range(n):\n",
    "        prompt = prompts[i]\n",
    "        res1, res2 = responses[i*2], responses[i*2 + 1]\n",
    "        a1, a2 = get_preference([prompt], [res1], [res2])\n",
    "        a1_list.append(a1)\n",
    "        a2_list.append(a2)\n",
    "    return {\n",
    "        \"prompt\": [[{\"role\": \"user\", \"content\": p}] for p in prompts],\n",
    "        \"a1\":     [[{\"role\": \"assistant\", \"content\": a[0]}] for a in a1_list],\n",
    "        \"a2\":     [[{\"role\": \"assistant\", \"content\": a[0]}] for a in a2_list],\n",
    "        \"rank\":   [1] * len(prompts),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "10768f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = ultrafeedback_ds.select_columns(['prompt']).select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "52c944f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00, 13.17it/s]\n",
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00, 13.78it/s]\n",
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00, 13.44it/s]\n",
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00, 13.31it/s]\n",
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00, 13.40it/s]\n",
      "Map: 100%|██████████| 5/5 [00:08<00:00,  1.64s/ examples]\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch, tokenizer = tokenizer, model = lm_model, temperature = 1.0):\n",
    "    responses = extract_dialogue(batch, tokenizer, model, temperature)['generated_response']\n",
    "    a1_list = []\n",
    "    a2_list = []\n",
    "    n = len(responses)//2\n",
    "    prompts = [p[0][\"content\"] for p in batch[\"prompt\"]]\n",
    "    for i in range(n):\n",
    "        prompt = prompts[i]\n",
    "        res1, res2 = responses[i*2], responses[i*2 + 1]\n",
    "        a1, a2 = get_preference([prompt], [res1], [res2])\n",
    "        a1_list.append(a1)\n",
    "        a2_list.append(a2)\n",
    "    return {\n",
    "        \"prompt\": [[{\"role\": \"user\", \"content\": p}] for p in prompts],\n",
    "        \"a1\":     [[{\"role\": \"assistant\", \"content\": a[0]}] for a in a1_list],\n",
    "        \"a2\":     [[{\"role\": \"assistant\", \"content\": a[0]}] for a in a2_list],\n",
    "        \"rank\":   [1] * len(prompts),\n",
    "    }\n",
    "\n",
    "df_temp = prompts.map(\n",
    "    prepare_dataset,\n",
    "    batched = True,\n",
    "    batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0976ed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 1025.00ba/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  3.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/august66/drpo_ultrafeedback_qwen2.5-1.5b/commit/45512c9f78edcc10cc1bf9442b9ef4110e9a2c69', commit_message='Upload dataset', commit_description='', oid='45512c9f78edcc10cc1bf9442b9ef4110e9a2c69', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/august66/drpo_ultrafeedback_qwen2.5-1.5b', endpoint='https://huggingface.co', repo_type='dataset', repo_id='august66/drpo_ultrafeedback_qwen2.5-1.5b'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.push_to_hub('august66/drpo_ultrafeedback_qwen2.5-1.5b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "19110874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'content': 'Sure! Below is a complete C++ program that does what you\\'ve asked for. It prompts the user to enter the name of a country and then checks whether that country borders the Mediterranean Sea.\\n\\n```cpp\\n#include <iostream>\\n#include <string>\\n\\nusing namespace std;\\n\\nint main() {\\n    string country;\\n    \\n    // Prompt user for input\\n    cout << \"Enter the name of a country: \";\\n    getline(cin, country); // Use getline instead of just cin\\n\\n    // Check if the entered country borders the Mediterranean Sea\\n    if (country == \"Italy\" || country == \"France\" || \\n        country == \"Greece\" || country == \"Cyprus\" || \\n        country == \"Spain\" || country == \"Albania\") {\\n        cout << \"The country \" << country << \" borders the Mediterranean Sea.\" << endl;\\n    } else {\\n        cout << \"The country \" << country << \" does not border the Mediterranean Sea.\" << endl;\\n    }\\n\\n    return 0;\\n}\\n```\\n\\n### Explanation:\\n\\n1. **Include Headers**: The `#include` directives at the beginning include the necessary libraries (`<iostream>` for input/output operations and `<string>` for handling strings).\\n2. **Namespace Declaration**: The `using',\n",
       "   'role': 'assistant'}],\n",
       " [{'content': 'Sure! Here\\'s an example of how I might use GPT to generate titles and descriptions for my YouTube videos:\\n1. Start with a list of video topics that I plan to cover in the upcoming week or month. For instance, let\\'s say I\\'m planning to create videos about self-care routines, productivity hacks, and personal development tips.\\n2. Using the list of video topics as input, I would run the following command on Google Colab:\\n```\\ngpt-3.5-turbo --prompt \"Please write 3 title ideas and 3 description ideas related to [self-care routine], [productivity hack] and [personal development tip].\" \\n```\\nThis would return three suggestions for each topic, including one catchy title and one informative description. \\nHere\\'s an example output:\\nTitle 1: \"Self-Care Routine Tips That Actually Work\"\\nDescription 1: \"Discover our top 5 self-care routines that will help you reduce stress, boost your mood, and make you feel better.\"\\nTitle 2: \"Productive Habits Every Working Professional Should Adopt\"\\nDescription 2: \"In this video, we\\'ll show you how to implement some of the most effective productivity habits into your daily routine.\"\\nTitle 3: \"How To',\n",
       "   'role': 'assistant'}],\n",
       " [{'content': 'The stock market crash of 1929 was caused by many interconnected economic, political, and social factors. One significant factor was the impact of World War I on the global economy. The war had led to increased demand for goods and services worldwide, which put upward pressure on prices and fueled inflation. This situation made it difficult for businesses to make profits, leading to reduced investments and consumer spending.\\n\\nAnother key factor was the Smoot-Hawley Tariff Act, signed into law by President Hoover in 1930. The act imposed high tariffs on imported goods, which raised prices for consumers and hurt American exporters. This protectionist policy also discouraged foreign trade, limiting access to cheaper imports and exacerbating domestic shortages.\\n\\nSpeculative investment practices and margin trading played an important role in fueling the boom before the crash. Many investors borrowed money from brokers to purchase stocks, using their own assets as collateral (a practice known as \"pennies on paper\"). When stock prices fell dramatically, these leveraged positions were often unsustainable, causing widespread bankruptcies and losses for investors who relied heavily on speculation.\\n\\nSocial and cultural changes during the Roaring Twenties contributed to the confidence that drove the stock market\\'s bubble. Americans enjoyed unprecedented prosperity, with rising incomes, technological innovations,',\n",
       "   'role': 'assistant'}],\n",
       " [{'content': 'Certainly! Here is the JavaScript code to convert the decimal number 31 to binary format:\\n```javascript\\nvar dec = 31;\\nvar bin = dec.toString(2);\\nconsole.log(bin); // Output: \"11111\"\\n```\\nThe `toString()` method with a second argument of `2` converts the decimal number to its binary representation as a string. In this case, the output will be `\"11111\"`, which represents the binary equivalent of the decimal number 31.\\nAlternatively, you can use the built-in `Math.pow()` function and some bitwise operations to perform the conversion more concisely:\\n```javascript\\nvar dec = 31;\\nvar bin = (dec >>> 0).toString(2);\\nconsole.log(bin); // Output: \"11111\"\\n```\\nThis code uses the right shift operator (`>>>`) to discard any lower-order bits in the integer that would cause an overflow when converted to binary. The result is then converted back to a string using `.toString()`.',\n",
       "   'role': 'assistant'}],\n",
       " [{'content': \"Yes, I can help you with that! Here's how we can modify the given C++ code to generate the first 20 Fibonacci numbers using the golden ratio:\\nStep 1: We need to understand the formula involving the golden ratio that generates Fibonacci numbers. The formula is as follows:\\nF(n) = φ^n / √5 * (φ^(n-1) - (-φ)^(-n+1))\\nwhere F(n) is the nth term of the Fibonacci sequence, φ is the golden ratio (approximately equal to 1.618), and n is the position of the term in the sequence.\\nStep 2: We need to create a table of values for the first 10 Fibonacci numbers to help us generate the rest. Here's an example table:\\n| Position | Fibonacci Number |\\n|----------|------------------|\\n| 0        | 0                |\\n| 1        | 1                |\\n| 2        | 1                |\\n| 3        | 2                |\\n| 4        | 3                |\\n| 5        | 5                |\\n| 6        | 8                |\\n| 7        | 13               |\\n| 8        | 21               |\\n|\",\n",
       "   'role': 'assistant'}]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp['a1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aeb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import HfArgumentParser, AutoTokenizer, AutoModelForCausalLM\n",
    "from trl.data_utils import apply_chat_template\n",
    "\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "METHOD_NAME = \"drpo-hh-1e-0066004\"\n",
    "MODEL_NAME = \"Eehan/pythia-1b-deduped-hh-drpo-base-1e-temp0.66-beta-0.04\"  \n",
    "OUTPUT_DATASET_NAME = \"Eehan/eval-hh\"\n",
    "INPUT_DATASET_NAME = \"Kyleyee/train_data_Helpful_explicit_prompt\"  \n",
    "INPUT_DATASET_SPLIT = \"test\"  \n",
    "DATASET_NEED_MERGE = \"Eehan/eval-hh\"\n",
    "TEMPERATURES = [0, 0.25, 0.5, 0.75, 1]  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "print(tokenizer.special_tokens_map)\n",
    "def generate_text(prompts, tokenizer, model, temperature):\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    generate_kwargs = {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"do_sample\": temperature > 0,\n",
    "    }\n",
    "    \n",
    "    if temperature > 0:\n",
    "        generate_kwargs[\"temperature\"] = temperature\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        **generate_kwargs\n",
    "    )\n",
    "    \n",
    "    generated_ids = outputs[:, inputs.input_ids.shape[1]:]\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "def truncate_human(texts):\n",
    "    return [text.split(\"\\n\\nHuman\")[0] for text in texts]\n",
    "\n",
    "def extract_dialogue(examples: dict, tokenizer, model, temperature: float) -> dict:\n",
    "    prompts = examples[\"prompt\"]\n",
    "    chat_prompts = [apply_chat_template({\"prompt\": p}, tokenizer) for p in prompts]\n",
    "    flat_prompts = [x[\"prompt\"] for x in chat_prompts]\n",
    "    responses = generate_text(flat_prompts, tokenizer, model, temperature)\n",
    "    responses = truncate_human(responses)\n",
    "    return {\n",
    "        \"generated_response\": responses\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset = load_dataset(INPUT_DATASET_NAME)[INPUT_DATASET_SPLIT]\n",
    "    dataset_merge = load_dataset(DATASET_NEED_MERGE)\n",
    "    dataset = dataset.remove_columns([\"rejected\", \"chosen\"])\n",
    "    # dataset_merge = dataset_merge.remove_columns([\"drdpo\", \"dpo_hinge\", \"drpo-hh-0.82e-0066004\"])\n",
    "\n",
    "    for temp in TEMPERATURES:\n",
    "        processed_shard = dataset.map(\n",
    "            lambda examples: extract_dialogue(examples, tokenizer, model, temp),\n",
    "            batched=True,\n",
    "            batch_size=64\n",
    "        )\n",
    "        dataset_merge[f\"temperature_{temp}\"] = dataset_merge[f\"temperature_{temp}\"].add_column(\n",
    "            METHOD_NAME,\n",
    "            processed_shard[\"generated_response\"]\n",
    "        )\n",
    "\n",
    "\n",
    "    dataset_merge.push_to_hub(OUTPUT_DATASET_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
