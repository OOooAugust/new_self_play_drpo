{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Self_play_DRPO/self_play_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from trl import DPOTrainer, DPOConfig, ModelConfig,get_quantization_config,get_kbit_device_map, \n",
    "\n",
    "# Load environment variables from /etc/network_turbo\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "def strip_prompt(prompt: str, text: str) -> str:\n",
    "    \"\"\"\n",
    "    If `text` literally starts with `prompt` (ignoring leading/trailing\n",
    "    whitespace), cut that prefix off and return the remainder.\n",
    "    \"\"\"\n",
    "    p = prompt.strip()\n",
    "    # Escaping safeguards punctuation / regex metacharacters\n",
    "    pattern = r\"^\\s*\" + re.escape(p) + r\"\\s*\"\n",
    "    return re.sub(pattern, \"\", text, count=1).lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "519b85fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nds_ultrafeed_train = load_dataset(\"openbmb/UltraFeedback\", split = \\'train\\', cache_dir=\"/root/autodl-tmp/dataset\")\\nds_ultrafeed_train = ds_ultrafeed_train.filter(lambda x: len(x[\\'completions\\']) > 0)\\ndef get_preferred(sample):\\n    prompt = sample[\\'instruction\\']\\n    sample_completions = sample[\\'completions\\']\\n    scores = [sample_completions[i][\\'overall_score\\'] for i in range(len(sample_completions))]\\n    responses = [sample_completions[i][\\'response\\'] for i in range(len(sample_completions))]\\n    preferred_index = scores.index(max(scores))\\n    dispreferred_index = scores.index(min(scores))\\n    preferred_ans = responses[preferred_index]\\n    dispreferred_ans = responses[dispreferred_index]\\n    return {\\n        \\'prompt\\':prompt,\\n        \\'preferred\\': preferred_ans,\\n        \\'dispreferred\\': dispreferred_ans,\\n    }\\n\\nprocessed_dataset = ds_ultrafeed_train.map(get_preferred, remove_columns=ds_ultrafeed_train.column_names)\\nprocessed_dataset.push_to_hub(\"august66/DRPO_data_from_ultrafeed\", split=\"train\")\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ds_ultrafeed_train = load_dataset(\"openbmb/UltraFeedback\", split = 'train', cache_dir=\"/root/autodl-tmp/dataset\")\n",
    "ds_ultrafeed_train = ds_ultrafeed_train.filter(lambda x: len(x['completions']) > 0)\n",
    "def get_preferred(sample):\n",
    "    prompt = sample['instruction']\n",
    "    sample_completions = sample['completions']\n",
    "    scores = [sample_completions[i]['overall_score'] for i in range(len(sample_completions))]\n",
    "    responses = [sample_completions[i]['response'] for i in range(len(sample_completions))]\n",
    "    preferred_index = scores.index(max(scores))\n",
    "    dispreferred_index = scores.index(min(scores))\n",
    "    preferred_ans = responses[preferred_index]\n",
    "    dispreferred_ans = responses[dispreferred_index]\n",
    "    return {\n",
    "        'prompt':prompt,\n",
    "        'preferred': preferred_ans,\n",
    "        'dispreferred': dispreferred_ans,\n",
    "    }\n",
    "    \n",
    "processed_dataset = ds_ultrafeed_train.map(get_preferred, remove_columns=ds_ultrafeed_train.column_names)\n",
    "processed_dataset.push_to_hub(\"august66/DRPO_data_from_ultrafeed\", split=\"train\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model_name = 'Qwen/Qwen2.5-0.5B-Instruct'   # use 0.5B model to test for now \n",
    "cache_path = \"/root/autodl-tmp/model_cache\"\n",
    "model_args = ModelConfig(model_name)\n",
    "model_torch_dtype = torch.float16\n",
    "model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    ")\n",
    "lm_model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    **model_kwargs,\n",
    "    cache_dir = cache_path\n",
    ").eval()\n",
    "\n",
    "lm_model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path, \n",
    "    padding_side = 'left', \n",
    "    use_fast = True,\n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    cache_dir = cache_path\n",
    ")\n",
    "\n",
    "if not lm_model_tokenizer.pad_token:\n",
    "    lm_model_tokenizer.pad_token = lm_model_tokenizer.eos_token\n",
    "\n",
    "print (model_torch_dtype)\n",
    "\n",
    "seed = 42\n",
    "FIRST = 20_000\n",
    "SECOND = 20_000\n",
    "ds_prompt_reshuffle = ds_prompt_train.shuffle(seed=seed)\n",
    "ds_prompt_split_1 = ds_prompt_reshuffle.select(range(FIRST))\n",
    "ds_prompt_split_2 = ds_prompt_reshuffle.select(range(FIRST, FIRST + SECOND))\n",
    "ds_prompt_split_3 = ds_prompt_reshuffle.select(range(FIRST + SECOND, len(ds_prompt_reshuffle)))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_prompts = [row[\"instruction\"] for row in batch]\n",
    "    enc = lm_model_tokenizer(\n",
    "        batch_prompts,\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    return enc.input_ids, enc.attention_mask, batch_prompts\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds_prompt_split_1,\n",
    "    batch_size = 256,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn,\n",
    "    num_workers = 4\n",
    ")\n",
    "\n",
    "lm_model_instance.to(device)\n",
    "prompt_completion_list = []\n",
    "for input_ids, attentio_mask, prompts in tqdm(loader):\n",
    "    input_ids, attentio_mask = input_ids.to(device), attentio_mask.to(device)\n",
    "    outputs = lm_model_instance.generate(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attentio_mask,\n",
    "        max_new_tokens = 128,\n",
    "        do_sample = True,\n",
    "        temperature = 1.0,\n",
    "        top_p = 1.0,\n",
    "        num_return_sequences = 2,\n",
    "        pad_token_id = lm_model_tokenizer.pad_token_id,\n",
    "        eos_token_id = lm_model_tokenizer.eos_token_id,\n",
    "    )\n",
    "    decoded_output = lm_model_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        raw1, raw2 = decoded_output[2 * i : 2 * i + 2]\n",
    "\n",
    "        # strip echoes\n",
    "        resp1 = strip_prompt(prompt, raw1)\n",
    "        resp2 = strip_prompt(prompt, raw2)\n",
    "\n",
    "        prompt_completion_list.append({\n",
    "            \"instruction\": prompt,\n",
    "            \"response\": [resp1, resp2],\n",
    "        })\n",
    "ds_prompt_completion = Dataset.from_list(prompt_completion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35471437",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "FIRST = 20_000\n",
    "SECOND = 20_000\n",
    "ds_prompt_reshuffle = ds_prompt_train.shuffle(seed=seed)\n",
    "ds_prompt_split_1 = ds_prompt_reshuffle.select(range(FIRST))\n",
    "ds_prompt_split_2 = ds_prompt_reshuffle.select(range(FIRST, FIRST + SECOND))\n",
    "ds_prompt_split_3 = ds_prompt_reshuffle.select(range(FIRST + SECOND, len(ds_prompt_reshuffle)))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_prompts = [row[\"instruction\"] for row in batch]\n",
    "    enc = lm_model_tokenizer(\n",
    "        batch_prompts,\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    return enc.input_ids, enc.attention_mask, batch_prompts\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds_prompt_split_1,\n",
    "    batch_size = 256,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn,\n",
    "    num_workers = 4\n",
    ")\n",
    "\n",
    "lm_model_instance.to(device)\n",
    "prompt_completion_list = []\n",
    "for input_ids, attentio_mask, prompts in tqdm(loader):\n",
    "    input_ids, attentio_mask = input_ids.to(device), attentio_mask.to(device)\n",
    "    outputs = lm_model_instance.generate(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attentio_mask,\n",
    "        max_new_tokens = 128,\n",
    "        do_sample = True,\n",
    "        temperature = 1.0,\n",
    "        top_p = 1.0,\n",
    "        num_return_sequences = 2,\n",
    "        pad_token_id = lm_model_tokenizer.pad_token_id,\n",
    "        eos_token_id = lm_model_tokenizer.eos_token_id,\n",
    "    )\n",
    "    decoded_output = lm_model_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        raw1, raw2 = decoded_output[2 * i : 2 * i + 2]\n",
    "\n",
    "        # strip echoes\n",
    "        resp1 = strip_prompt(prompt, raw1)\n",
    "        resp2 = strip_prompt(prompt, raw2)\n",
    "\n",
    "        prompt_completion_list.append({\n",
    "            \"instruction\": prompt,\n",
    "            \"response\": [resp1, resp2],\n",
    "        })\n",
    "ds_prompt_completion = Dataset.from_list(prompt_completion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e90f2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "FIRST = 20_000\n",
    "SECOND = 20_000\n",
    "data_cache_path = \"/root/autodl-tmp/dataset\"\n",
    "drpo_train = load_dataset(\"august66/DRPO_data_from_ultrafeed\", split=\"train\", cache_dir=data_cache_path)\n",
    "drpo_train_reshuffle = drpo_train.shuffle(seed=seed)\n",
    "drpo_train_split_1 = drpo_train_reshuffle.select(range(FIRST))\n",
    "drpo_train_split_2 = drpo_train_reshuffle.select(range(FIRST, FIRST + SECOND))\n",
    "drpo_train_split_3 = drpo_train_reshuffle.select(range(FIRST + SECOND, len(drpo_train_reshuffle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990cefc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e721c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "self_play_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
