{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e90f2b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Self_play_DRPO/envs/my_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Extracting prompt in train dataset: 100%|██████████| 3/3 [00:00<00:00, 81.34 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 3/3 [00:00<00:00, 31.01 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after chat template dataset sample: {'prompt': 'You will be given a definition of a task first, then some input of the task.\\nIn this task, you are given a hateful post in Bengali that expresses hate or encourages violence towards a person or a group based on the protected characteristics such as race, religion, sex, and sexual orientation. You are expected to classify the post into two classes: political or non-political depending on the topic.\\n\\nকী আর বলব মামানমারে মুছুলমান মারছে আর আমাদের সরকার ভারতের টেরেন হাতছা।দুঃখ প্রকাশ করেছেন\\nOutput:', 'a1': 'Looking at the post, it appears like a political post with multiple expressions of hate towards different groups of people. Additionally, the use of profanity and inciting violence can also be seen as hateful speech. Can I provide any further assistance with this task?', 'a2': 'User, I understand that you need my help in categorizing a post into political or non-political based on the topic. Please provide me the post so I can analyze it for you.', 'rank': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|██████████| 3/3 [00:00<00:00, 114.79 examples/s]\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=12.290770530700684, metrics={'train_runtime': 70.4303, 'train_samples_per_second': 0.043, 'train_steps_per_second': 0.014, 'total_flos': 0.0, 'train_loss': 12.290770530700684, 'epoch': 1.0})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys, pathlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llm_blender.pair_ranker.pairrm import DebertaV2PairRM\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "LOCAL_TRL_PARENT = \"/workspace/Self_play_DRPO\"\n",
    "if LOCAL_TRL_PARENT not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_TRL_PARENT)\n",
    "\n",
    "    \n",
    "# now the import will use your local copy:\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    ModelConfig,\n",
    "    DRPOTrainer,\n",
    "    DRPOConfig,\n",
    ")\n",
    "\n",
    "from trl.trainer.drpo_utils import GPMwithRewardNetwork, estDPOStylePipeline, BTRewardNetwork, PairRMPipeline\n",
    "\n",
    "def strip_prompt(prompt: str, text: str) -> str:\n",
    "    \"\"\"\n",
    "    If `text` literally starts with `prompt` (ignoring leading/trailing\n",
    "    whitespace), cut that prefix off and return the remainder.\n",
    "    \"\"\"\n",
    "    p = prompt.strip()\n",
    "    # Escaping safeguards punctuation / regex metacharacters\n",
    "    pattern = r\"^\\s*\" + re.escape(p) + r\"\\s*\"\n",
    "    return re.sub(pattern, \"\", text, count=1).lstrip()\n",
    "\n",
    "seed = 42\n",
    "FIRST = 3\n",
    "SECOND = 20_000\n",
    "data_cache_path = \"/workspace/dataset\"\n",
    "drpo_train = load_dataset(\"august66/DRPO_data_from_ultrafeed\", split=\"train\", cache_dir=data_cache_path)\n",
    "\n",
    "\n",
    "def process_split(original):\n",
    "    swapped = original.map(lambda x: {\n",
    "        'a1': x['a2'],\n",
    "        'a2': x['a1'],\n",
    "        # 'rank': 1 - int(random.random() < x['chosen_preference']),\n",
    "        'rank': 1 - x['rank'],\n",
    "    })\n",
    "\n",
    "    return concatenate_datasets([original, swapped]).shuffle(seed=seed)\n",
    "drpo_train = process_split(drpo_train)\n",
    "drpo_train_reshuffle = drpo_train.shuffle(seed=seed)\n",
    "drpo_train_split_1 = drpo_train_reshuffle.select(range(FIRST))\n",
    "drpo_train_split_2 = drpo_train_reshuffle.select(range(FIRST, FIRST + SECOND))\n",
    "drpo_train_split_3 = drpo_train_reshuffle.select(range(FIRST + SECOND, len(drpo_train_reshuffle)))\n",
    "\n",
    "device = 'cuda'\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"   # use 0.5B model to test for now \n",
    "cache_path = \"/workspace/model_cache\"\n",
    "model_args = ModelConfig(model_name)\n",
    "model_torch_dtype = torch.float16\n",
    "model_args.trust_remote_code = True\n",
    "model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    ")\n",
    "lm_model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    **model_kwargs,\n",
    "    cache_dir = cache_path,\n",
    ")\n",
    "\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    **model_kwargs,\n",
    "    cache_dir = cache_path,\n",
    ")\n",
    "\n",
    "lm_model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path, \n",
    "    padding_side = 'left', \n",
    "    use_fast = True,\n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    cache_dir = cache_path\n",
    ")\n",
    "\n",
    "if not lm_model_tokenizer.pad_token:\n",
    "    lm_model_tokenizer.pad_token = lm_model_tokenizer.eos_token\n",
    "\n",
    "\n",
    "with open(\"/workspace/Self_play_DRPO/DRPO_scripts/hh/train_configs/config_gpm.yaml\", \"r\") as f:\n",
    "    training_args_config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "training_args = DRPOConfig(\n",
    "    **training_args_config\n",
    ")\n",
    "\n",
    "\n",
    "training_args.preference_model_id = 'llm-blender/PairRM-hf'\n",
    "\n",
    "preference_pipeline = PairRMPipeline(\n",
    "    model_name_or_path = training_args.preference_model_id,\n",
    ")\n",
    "\n",
    "trainer = DRPOTrainer(\n",
    "    model=lm_model_instance,\n",
    "    ref_model=ref_model,\n",
    "    preference_model=preference_pipeline,\n",
    "    train_dataset = drpo_train_split_1,\n",
    "    processing_class=lm_model_tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6806ed09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'explain methods to optimize sql queries with use cases and code',\n",
       " 'a1': 'Methods to optimize SQL queries include using indexes, clustering, using proper query formatting and syntax, and avoiding using unnecessary keywords. Indexes can help quickly locate data in the database, which in turn can improve query performance. Clustering physical databases can help organize related data in a specific area of the hard drive resulting in increased query speed and reduced resource usage. Queries should be written carefully to ensure that the most appropriate information is targeted. Moreover avoiding unnecessary keywords or special characters that are not relevant to the query can reduce the resources used and maximize the efficiency of the query being run.',\n",
       " 'a2': \"Sure, I'd be happy to help you understand methods to optimize SQL queries with some use cases and sample code.\\n\\nStep 1: Analyze Query Execution Plan\\nThe first step in optimizing an SQL query is to analyze the execution plan. The execution plan is a tree-like structure that shows how the database engine will generate the final result. The query optimizer analyzes the query and generates the most efficient execution plan possible. However, it isn't always perfect, and there may be ways to improve upon it.\\n\\nOne way to analyze the execution plan is by using the EXPLAIN PLAN statement. This statement shows the steps the database engine takes to generate the result, including the specific index usage, table scans, and other details.\\n\\nUse case: Suppose you have a large e-commerce website with a lot of traffic. You notice that queries are taking a long time to execute, and you suspect that the database is becoming a bottleneck. You can use the EXPLAIN PLAN statement to analyze the execution plan and identify any areas where improvements can be made.\\n\\nSample code: `EXPLAIN PLAN FOR <<your query here>>;`\\n\\nStep 2: Optimize Database Schema\\nThe database schema design can have a significant impact on query performance. For example, poorly designed tables or indexes can result in excessive overhead and slower query execution times.\\n\\nOne way to optimize the database schema is by using normalization. Normalization involves breaking down data into smaller tables and ensuring that there is no redundant data. This can improve query performance by eliminating unnecessary joins and reducing the amount of data that needs to be processed.\\n\\nUse case: Suppose you are working on a data warehousing project where you need to store and analyze large amounts of data. You notice that some of the queries are taking a long time to execute. You can optimize the database schema by normalizing the tables and creating appropriate indexes.\\n\\nSample code: This isn't a standalone code snippet, but rather involves redesigning your database schema to be more efficient.\\n\\nStep 3: Use Indexes Intelligently\\nIndexes are used to speed up queries by allowing the database engine to find the necessary data more quickly. However, creating too many indexes or using them improperly can have the opposite effect and actually slow down queries.\\n\\nTo use indexes intelligently, you should:\\n\\n* Create indexes on columns that are used in WHERE, JOIN, and ORDER BY clauses.\\n* Avoid creating indexes on columns with low selectivity.\\n* Monitor and maintain indexes regularly to ensure they are still useful.\\n\\nUse case: Suppose you have a database with a large number of customers, and you need to frequently query for customers with a specific location or specific product preferences. You can create indexes on the relevant columns to speed up the queries.\\n\\nSample code: `CREATE INDEX idx_location_city ON customers (location, city);`\\n\\nStep 4: Use Subqueries and Joins Wisely\\nSubqueries and joins can be useful in SQL queries, but they can also be resource-intensive and slow down performance. You should use subqueries and joins wisely and limit their usage when possible.\\n\\nUse case: Suppose you have a database with customers and orders. You need to find the total revenue for each customer within a specific date range. By using a subquery, you can limit the number of orders retrieved from the database, thereby improving query performance.\\n\\nSample code: `SELECT customer_id, SUM(total) AS revenue FROM orders WHERE order_date BETWEEN '2021-01-01' AND '2021-12-31' GROUP BY customer_id;`\\n\\nWith these methods, you can analyze and optimize your SQL queries for improved performance. Please keep in mind that the optimal solution varies depending on your specific use case and your database's structure.\",\n",
       " 'rank': 0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drpo_train_split_2[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb60f106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dir': './output/hh/gpm/',\n",
       " 'gradient_checkpointing': False,\n",
       " 'model_and_preference_share_basemodel': False,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 16,\n",
       " 'learning_rate': 5e-07,\n",
       " 'max_length': 1024,\n",
       " 'generate_temperature': 0.5,\n",
       " 'beta': 0.04,\n",
       " 'bf16': True,\n",
       " 'dataset_num_proc': 1,\n",
       " 'num_astar': 2,\n",
       " 'torch_empty_cache_steps': 1,\n",
       " 'num_train_epochs': 1,\n",
       " 'eval_steps': 500,\n",
       " 'eval_strategy': 'no',\n",
       " 'save_strategy': 'steps',\n",
       " 'save_steps': 1000,\n",
       " 'logging_steps': 5,\n",
       " 'push_to_hub': False,\n",
       " 'hub_model_id': 'Eehan/nothing_really_matters',\n",
       " 'report_to': ['none'],\n",
       " 'is_bt_model': False,\n",
       " 'preference_model_id': 'Kyleyee/gpm_tldr_3e',\n",
       " 'preference_model_kwargs': {'indifferent': False,\n",
       "  'random': False,\n",
       "  'reverse': False},\n",
       " 'ratio_processing': 'clip',\n",
       " 'clipbound': 2.5,\n",
       " 'forward_temperature': 0.5,\n",
       " 'max_grad_norm': 0.25,\n",
       " 'loss1_only': False,\n",
       " 'loss2_only': False}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0b86d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
