{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e1f252f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import torch\n",
    "import sys, pathlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llm_blender.pair_ranker.pairrm import DebertaV2PairRM\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "LOCAL_TRL_PARENT = \"/root/autodl-tmp/Self_play_DRPO\"\n",
    "if LOCAL_TRL_PARENT not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_TRL_PARENT)\n",
    "\n",
    "    \n",
    "# now the import will use your local copy:\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    ModelConfig,\n",
    "    DRPOTrainer,\n",
    "    DRPOConfig,\n",
    ")\n",
    "\n",
    "from trl.trainer.drpo_utils import GPMwithRewardNetwork, estDPOStylePipeline, BTRewardNetwork, PariRMPipeline\n",
    "\n",
    "# Load environment variables from /etc/network_turbo\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "def strip_prompt(prompt: str, text: str) -> str:\n",
    "    \"\"\"\n",
    "    If `text` literally starts with `prompt` (ignoring leading/trailing\n",
    "    whitespace), cut that prefix off and return the remainder.\n",
    "    \"\"\"\n",
    "    p = prompt.strip()\n",
    "    # Escaping safeguards punctuation / regex metacharacters\n",
    "    pattern = r\"^\\s*\" + re.escape(p) + r\"\\s*\"\n",
    "    return re.sub(pattern, \"\", text, count=1).lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "519b85fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nds_ultrafeed_train = load_dataset(\"openbmb/UltraFeedback\", split = \\'train\\', cache_dir=\"/root/autodl-tmp/dataset\")\\nds_ultrafeed_train = ds_ultrafeed_train.filter(lambda x: len(x[\\'completions\\']) > 0)\\ndef get_preferred(sample):\\n    prompt = sample[\\'instruction\\']\\n    sample_completions = sample[\\'completions\\']\\n    scores = [sample_completions[i][\\'overall_score\\'] for i in range(len(sample_completions))]\\n    responses = [sample_completions[i][\\'response\\'] for i in range(len(sample_completions))]\\n    preferred_index = scores.index(max(scores))\\n    dispreferred_index = scores.index(min(scores))\\n    preferred_ans = responses[preferred_index]\\n    dispreferred_ans = responses[dispreferred_index]\\n    return {\\n        \\'prompt\\':prompt,\\n        \\'preferred\\': preferred_ans,\\n        \\'dispreferred\\': dispreferred_ans,\\n    }\\n\\nprocessed_dataset = ds_ultrafeed_train.map(get_preferred, remove_columns=ds_ultrafeed_train.column_names)\\nprocessed_dataset.push_to_hub(\"august66/DRPO_data_from_ultrafeed\", split=\"train\")\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "ds_ultrafeed_train = load_dataset(\"openbmb/UltraFeedback\", split = 'train', cache_dir=\"/root/autodl-tmp/dataset\")\n",
    "ds_ultrafeed_train = ds_ultrafeed_train.filter(lambda x: len(x['completions']) > 0)\n",
    "def get_preferred(sample):\n",
    "    prompt = sample['instruction']\n",
    "    sample_completions = sample['completions']\n",
    "    scores = [sample_completions[i]['overall_score'] for i in range(len(sample_completions))]\n",
    "    responses = [sample_completions[i]['response'] for i in range(len(sample_completions))]\n",
    "    preferred_index = scores.index(max(scores))\n",
    "    dispreferred_index = scores.index(min(scores))\n",
    "    preferred_ans = responses[preferred_index]\n",
    "    dispreferred_ans = responses[dispreferred_index]\n",
    "    return {\n",
    "        'prompt':prompt,\n",
    "        'preferred': preferred_ans,\n",
    "        'dispreferred': dispreferred_ans,\n",
    "    }\n",
    "    \n",
    "processed_dataset = ds_ultrafeed_train.map(get_preferred, remove_columns=ds_ultrafeed_train.column_names)\n",
    "processed_dataset.push_to_hub(\"august66/DRPO_data_from_ultrafeed\", split=\"train\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model_name = 'Qwen/Qwen2.5-0.5B-Instruct'   # use 0.5B model to test for now \n",
    "cache_path = \"/root/autodl-tmp/model_cache\"\n",
    "model_args = ModelConfig(model_name)\n",
    "model_torch_dtype = torch.float16\n",
    "model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    ")\n",
    "lm_model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    **model_kwargs,\n",
    "    cache_dir = cache_path\n",
    ").eval()\n",
    "\n",
    "lm_model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path, \n",
    "    padding_side = 'left', \n",
    "    use_fast = True,\n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    cache_dir = cache_path\n",
    ")\n",
    "\n",
    "if not lm_model_tokenizer.pad_token:\n",
    "    lm_model_tokenizer.pad_token = lm_model_tokenizer.eos_token\n",
    "\n",
    "print (model_torch_dtype)\n",
    "\n",
    "seed = 42\n",
    "FIRST = 20_000\n",
    "SECOND = 20_000\n",
    "ds_prompt_reshuffle = ds_prompt_train.shuffle(seed=seed)\n",
    "ds_prompt_split_1 = ds_prompt_reshuffle.select(range(FIRST))\n",
    "ds_prompt_split_2 = ds_prompt_reshuffle.select(range(FIRST, FIRST + SECOND))\n",
    "ds_prompt_split_3 = ds_prompt_reshuffle.select(range(FIRST + SECOND, len(ds_prompt_reshuffle)))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_prompts = [row[\"instruction\"] for row in batch]\n",
    "    enc = lm_model_tokenizer(\n",
    "        batch_prompts,\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    return enc.input_ids, enc.attention_mask, batch_prompts\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds_prompt_split_1,\n",
    "    batch_size = 256,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn,\n",
    "    num_workers = 4\n",
    ")\n",
    "\n",
    "lm_model_instance.to(device)\n",
    "prompt_completion_list = []\n",
    "for input_ids, attentio_mask, prompts in tqdm(loader):\n",
    "    input_ids, attentio_mask = input_ids.to(device), attentio_mask.to(device)\n",
    "    outputs = lm_model_instance.generate(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attentio_mask,\n",
    "        max_new_tokens = 128,\n",
    "        do_sample = True,\n",
    "        temperature = 1.0,\n",
    "        top_p = 1.0,\n",
    "        num_return_sequences = 2,\n",
    "        pad_token_id = lm_model_tokenizer.pad_token_id,\n",
    "        eos_token_id = lm_model_tokenizer.eos_token_id,\n",
    "    )\n",
    "    decoded_output = lm_model_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        raw1, raw2 = decoded_output[2 * i : 2 * i + 2]\n",
    "\n",
    "        # strip echoes\n",
    "        resp1 = strip_prompt(prompt, raw1)\n",
    "        resp2 = strip_prompt(prompt, raw2)\n",
    "\n",
    "        prompt_completion_list.append({\n",
    "            \"instruction\": prompt,\n",
    "            \"response\": [resp1, resp2],\n",
    "        })\n",
    "ds_prompt_completion = Dataset.from_list(prompt_completion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35471437",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "FIRST = 20_000\n",
    "SECOND = 20_000\n",
    "ds_prompt_reshuffle = ds_prompt_train.shuffle(seed=seed)\n",
    "ds_prompt_split_1 = ds_prompt_reshuffle.select(range(FIRST))\n",
    "ds_prompt_split_2 = ds_prompt_reshuffle.select(range(FIRST, FIRST + SECOND))\n",
    "ds_prompt_split_3 = ds_prompt_reshuffle.select(range(FIRST + SECOND, len(ds_prompt_reshuffle)))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch_prompts = [row[\"instruction\"] for row in batch]\n",
    "    enc = lm_model_tokenizer(\n",
    "        batch_prompts,\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 512,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    return enc.input_ids, enc.attention_mask, batch_prompts\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds_prompt_split_1,\n",
    "    batch_size = 256,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_fn,\n",
    "    num_workers = 4\n",
    ")\n",
    "\n",
    "lm_model_instance.to(device)\n",
    "prompt_completion_list = []\n",
    "for input_ids, attentio_mask, prompts in tqdm(loader):\n",
    "    input_ids, attentio_mask = input_ids.to(device), attentio_mask.to(device)\n",
    "    outputs = lm_model_instance.generate(\n",
    "        input_ids = input_ids,\n",
    "        attention_mask = attentio_mask,\n",
    "        max_new_tokens = 128,\n",
    "        do_sample = True,\n",
    "        temperature = 1.0,\n",
    "        top_p = 1.0,\n",
    "        num_return_sequences = 2,\n",
    "        pad_token_id = lm_model_tokenizer.pad_token_id,\n",
    "        eos_token_id = lm_model_tokenizer.eos_token_id,\n",
    "    )\n",
    "    decoded_output = lm_model_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        raw1, raw2 = decoded_output[2 * i : 2 * i + 2]\n",
    "\n",
    "        # strip echoes\n",
    "        resp1 = strip_prompt(prompt, raw1)\n",
    "        resp2 = strip_prompt(prompt, raw2)\n",
    "\n",
    "        prompt_completion_list.append({\n",
    "            \"instruction\": prompt,\n",
    "            \"response\": [resp1, resp2],\n",
    "        })\n",
    "ds_prompt_completion = Dataset.from_list(prompt_completion_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90f2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "FIRST = 100\n",
    "SECOND = 20_000\n",
    "data_cache_path = \"/root/autodl-tmp/dataset\"\n",
    "drpo_train = load_dataset(\"august66/DRPO_data_from_ultrafeed\", split=\"train\", cache_dir=data_cache_path)\n",
    "drpo_train = drpo_train.rename_columns({\n",
    "    \"preferred\": \"a1\",\n",
    "    \"dispreferred\":\"a2\",\n",
    "})\n",
    "ones = [1] * drpo_train.num_rows \n",
    "drpo_train = drpo_train.add_column(\"rank\", ones)\n",
    "\n",
    "\n",
    "def process_split(original):\n",
    "    swapped = original.map(lambda x: {\n",
    "        'a1': x['a2'],\n",
    "        'a2': x['a1'],\n",
    "        # 'rank': 1 - int(random.random() < x['chosen_preference']),\n",
    "        'rank': 1 - x['rank'],\n",
    "    })\n",
    "\n",
    "    return concatenate_datasets([original, swapped]).shuffle(seed=seed)\n",
    "drpo_train = process_split(drpo_train)\n",
    "drpo_train_reshuffle = drpo_train.shuffle(seed=seed)\n",
    "drpo_train_split_1 = drpo_train_reshuffle.select(range(FIRST))\n",
    "drpo_train_split_2 = drpo_train_reshuffle.select(range(FIRST, FIRST + SECOND))\n",
    "drpo_train_split_3 = drpo_train_reshuffle.select(range(FIRST + SECOND, len(drpo_train_reshuffle)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "model_name = \"Qwen/Qwen2-0.5B-Instruct\"   # use 0.5B model to test for now \n",
    "cache_path = \"/root/autodl-tmp/model_cache\"\n",
    "model_args = ModelConfig(model_name)\n",
    "model_torch_dtype = torch.float16\n",
    "model_args.trust_remote_code = True\n",
    "model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    ")\n",
    "lm_model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    **model_kwargs,\n",
    "    cache_dir = cache_path,\n",
    ").to('cpu')\n",
    "\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    **model_kwargs,\n",
    "    cache_dir = cache_path,\n",
    ").to('cpu')\n",
    "\n",
    "lm_model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path, \n",
    "    padding_side = 'left', \n",
    "    use_fast = True,\n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    cache_dir = cache_path\n",
    ")\n",
    "\n",
    "if not lm_model_tokenizer.pad_token:\n",
    "    lm_model_tokenizer.pad_token = lm_model_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e721c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files: 100%|██████████| 8/8 [00:00<00:00, 87838.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXConfig {\n",
      "  \"architectures\": [\n",
      "    \"CustomRewardModel\"\n",
      "  ],\n",
      "  \"attention_bias\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"gpt_neox\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"partial_rotary_factor\": 0.25,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"rotary_emb_base\": 10000,\n",
      "  \"rotary_pct\": 0.25,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_parallel_residual\": true,\n",
      "  \"vocab_size\": 50304\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt in train dataset: 100%|██████████| 100/100 [00:00<00:00, 6861.85 examples/s]\n",
      "Applying chat template to train dataset: 100%|██████████| 100/100 [00:00<00:00, 8604.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after chat template dataset sample: {'prompt': 'You will be given a definition of a task first, then some input of the task.\\nIn this task, you are given a hateful post in Bengali that expresses hate or encourages violence towards a person or a group based on the protected characteristics such as race, religion, sex, and sexual orientation. You are expected to classify the post into two classes: political or non-political depending on the topic.\\n\\nকী আর বলব মামানমারে মুছুলমান মারছে আর আমাদের সরকার ভারতের টেরেন হাতছা।দুঃখ প্রকাশ করেছেন\\nOutput:', 'a1': 'Looking at the post, it appears like a political post with multiple expressions of hate towards different groups of people. Additionally, the use of profanity and inciting violence can also be seen as hateful speech. Can I provide any further assistance with this task?', 'a2': 'User, I understand that you need my help in categorizing a post into political or non-political based on the topic. Please provide me the post so I can analyze it for you.', 'rank': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing train dataset: 100%|██████████| 100/100 [00:00<00:00, 510.24 examples/s]\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [254,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/Indexing.cu:1553: indexSelectLargeIndex: block: [255,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     22\u001b[39m     preference_pipeline = GPMwithRewardNetwork(training_args.preference_model_id)\n\u001b[32m     24\u001b[39m trainer = DRPOTrainer(\n\u001b[32m     25\u001b[39m     model=lm_model_instance,\n\u001b[32m     26\u001b[39m     ref_model=ref_model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     args=training_args,\n\u001b[32m     31\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/trl/trainer/drpo_trainer.py:693\u001b[39m, in \u001b[36mDRPOTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m    691\u001b[39m     prompt_a2_repeated_ids = prompt_a2_ids.repeat(\u001b[38;5;28mself\u001b[39m.args.num_astar, \u001b[32m1\u001b[39m)\n\u001b[32m    692\u001b[39m     prompt_a2_repeated_attention_mask = prompt_a2_attention_mask.repeat(\u001b[38;5;28mself\u001b[39m.args.num_astar, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m     preference_score_star = \u001b[43mget_preference_score_without_decoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreference_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_astar_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_astar_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_a2_repeated_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_a2_repeated_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_bt_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_bt_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreference_model_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.missing_eos_penalty \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    703\u001b[39m         preference_score_star[~contain_eos_token] -= \u001b[38;5;28mself\u001b[39m.args.missing_eos_penalty\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/trl/trainer/drpo_utils.py:440\u001b[39m, in \u001b[36mget_preference_score_without_decoding\u001b[39m\u001b[34m(preference_model, a1_iuput_ids, a1_attention_mask, a2_input_ids, a2_attention_mask, is_bt_model, kwargs, device)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    438\u001b[39m     \u001b[38;5;66;03m# return (torch.rand(len(a_1_iuput)) - 0.5 * torch.ones(len(a_1_iuput))).to(device)\u001b[39;00m\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.rand(\u001b[38;5;28mlen\u001b[39m(a1_iuput_ids)).to(device)\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m a1_reward = \u001b[43mpreference_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1_iuput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma1_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m a2_reward = preference_model(a2_input_ids, a2_attention_mask)\n\u001b[32m    442\u001b[39m \u001b[38;5;66;03m# print(\"a1_reward:\", a1_reward.shape, a1_reward)\u001b[39;00m\n\u001b[32m    443\u001b[39m \u001b[38;5;66;03m# print(\"a2_reward:\", a2_reward.shape, a2_reward)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/trl/trainer/drpo_utils.py:170\u001b[39m, in \u001b[36mGPMwithRewardNetwork.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, return_output)\u001b[39m\n\u001b[32m    168\u001b[39m attention_mask[:, -\u001b[32m1\u001b[39m] = \u001b[32m1\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     rewards, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcustom_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rewards\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/trl/trainer/drpo_utils.py:50\u001b[39m, in \u001b[36mget_reward_model.<locals>.CustomRewardModel.custom_forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, return_output)\u001b[39m\n\u001b[32m     48\u001b[39m position_ids = attention_mask.long().cumsum(-\u001b[32m1\u001b[39m) - \u001b[32m1\u001b[39m\n\u001b[32m     49\u001b[39m position_ids.masked_fill_(attention_mask == \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m outputs = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model_prefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m last_hidden_states = outputs[\u001b[33m\"\u001b[39m\u001b[33mlast_hidden_state\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_general_preference:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:412\u001b[39m, in \u001b[36mGPTNeoXModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    410\u001b[39m     position_ids = cache_position.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m causal_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    417\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    418\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    421\u001b[39m converted_head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:527\u001b[39m, in \u001b[36mGPTNeoXModel._update_causal_mask\u001b[39m\u001b[34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[39m\n\u001b[32m    520\u001b[39m     target_length = (\n\u001b[32m    521\u001b[39m         attention_mask.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m    522\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(attention_mask, torch.Tensor)\n\u001b[32m    523\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m past_seen_tokens + sequence_length + \u001b[32m1\u001b[39m\n\u001b[32m    524\u001b[39m     )\n\u001b[32m    526\u001b[39m \u001b[38;5;66;03m# In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m causal_mask = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_4d_causal_attention_mask_with_cache_position\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    537\u001b[39m     \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    538\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    543\u001b[39m     \u001b[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[32m    544\u001b[39m     \u001b[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[32m    545\u001b[39m     min_dtype = torch.finfo(dtype).min\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/Self_play_DRPO/env/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:589\u001b[39m, in \u001b[36mGPTNeoXModel._prepare_4d_causal_attention_mask_with_cache_position\u001b[39m\u001b[34m(attention_mask, sequence_length, target_length, dtype, cache_position, batch_size, **kwargs)\u001b[39m\n\u001b[32m    585\u001b[39m causal_mask = torch.full(\n\u001b[32m    586\u001b[39m     (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n\u001b[32m    587\u001b[39m )\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sequence_length != \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m     causal_mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtriu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagonal\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    590\u001b[39m causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    591\u001b[39m causal_mask = causal_mask[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :, :].expand(batch_size, \u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(\"/root/autodl-tmp/Self_play_DRPO/DRPO_scripts/hh/train_configs/config_gpm.yaml\", \"r\") as f:\n",
    "    training_args_config = yaml.safe_load(f)\n",
    "\n",
    "\n",
    "training_args = DRPOConfig(\n",
    "    **training_args_config\n",
    ")\n",
    "\"\"\"\n",
    "training_args.preference_model_id = 'llm-blender/PairRM-hf'\n",
    "\n",
    "preference_pipeline = PariRMPipeline(\n",
    "    model_name_or_path = training_args.preference_model_id,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "if training_args.is_bt_model:\n",
    "    if isinstance(training_args.preference_model_id, dict):\n",
    "        preference_pipeline = estDPOStylePipeline(training_args.preference_model_id)\n",
    "    else: \n",
    "        preference_pipeline = BTRewardNetwork(training_args.preference_model_id, revision=training_args.preference_model_revision)\n",
    "else:\n",
    "    preference_pipeline = GPMwithRewardNetwork(training_args.preference_model_id)\n",
    "\n",
    "trainer = DRPOTrainer(\n",
    "    model=lm_model_instance,\n",
    "    ref_model=ref_model,\n",
    "    preference_model=preference_pipeline ,\n",
    "    train_dataset = drpo_train_split_1,\n",
    "    processing_class=lm_model_tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94dcf336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<trl.trainer.drpo_trainer.DRPOTrainer at 0x7f20d689d160>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05cb93fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No ranker config provided, no ranker loaded, please load ranker first through load_ranker()\n",
      "WARNING:root:No fuser config provided, no fuser loaded, please load fuser first through load_fuser()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded ranker from  /root/.cache/huggingface/hub/llm-blender/PairRM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ranking candidates: 100%|██████████| 1/1 [00:00<00:00, 29.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.898]\n",
      "[ True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import llm_blender\n",
    "blender = llm_blender.Blender()\n",
    "# Load Ranker\n",
    "blender.loadranker(\"llm-blender/PairRM\") # load ranker checkpoint\n",
    "inputs = [\"hello!\"]\n",
    "candidates_A = [\"hi!\"]\n",
    "candidates_B = [\"f**k off!\"]\n",
    "logits = blender.compare(inputs, candidates_A, candidates_B, return_logits=True, mode=\"[A,B]\")\n",
    "comparison_results = logits > 0\n",
    "print(logits)\n",
    "# [ 1.9   -1.255]\n",
    "print(comparison_results)\n",
    "# tensor([ True, False], device='cuda:0'), which means whether candidate A is better than candidate B for each input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c728f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairrm = PariRMPipeline(\n",
    "    model_name_or_path = 'llm-blender/PairRM-hf',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "237901c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\"hello!\", \"I love you!\"]\n",
    "candidates_A = [\"hi!\", \"I hate you!\"]\n",
    "candidates_B = [\"f**k off!\", \"I love you, too!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074c8801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8699, 0.2219])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(pairrm(inputs, candidates_A, candidates_B)).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9c9cb85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llm_blender.pair_ranker.pairrm.DebertaV2PairRM"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pairrm.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88fa0173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(pairrm.model, DebertaV2PairRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e0d1224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8699, 0.2219])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl.trainer.drpo_utils import get_preference_score\n",
    "get_preference_score(pairrm, candidates_A, candidates_B, inputs = inputs, kwargs = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7048763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drpo_train_split_1_test = drpo_train_split_1.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd3e1bec",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DRPOConfig' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtraining_args\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreport\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'DRPOConfig' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "training_args['report']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66685f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'preferred', 'dispreferred'],\n",
       "    num_rows: 63966\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drpo_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e5f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpm_model = 'Kyleyee/gpm_tldr_3e'\n",
    "gpm = AutoModelForCausalLM.from_pretrained(\n",
    "    gpm_model,\n",
    "    cache_dir = cache_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386e4d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXForCausalLM"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b38600de",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m gpm(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ma\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mValueError\u001b[39m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "gpm(torch.tensor(['a']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec678e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
