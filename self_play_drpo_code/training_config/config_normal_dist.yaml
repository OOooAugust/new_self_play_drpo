output_dir: ./output/hh/0/

gradient_checkpointing: false
model_and_preference_share_basemodel: false
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
learning_rate: 1.0e-6
max_new_tokens: 512
max_length: 1024
generate_temperature: 1.0
beta: 0.03
bf16: true
dataset_num_proc: 1
num_astar: 1
torch_empty_cache_steps: 1
num_train_epochs: 1
eval_steps: 500
eval_strategy: "no"
hub_strategy: "every_save"
save_strategy: "steps"
save_steps: 1000
logging_steps: 5
push_to_hub: true
hub_model_id: 'august66/hh_qwen1.5_drpo'
save_only_model: false
save_total_limit: 2
report_to: 
  - wandb
is_bt_model: true
preference_model_id: 'Kyleyee/Qwen2.5-1.5B-reward-hh-retrain'
#preference_model_revision: f70cf091d59583749f030005b70834d29ba70fda dd
preference_model_kwargs:
  indifferent: false
  random: false
  reverse: false
ratio_processing: clip
ratio_distribution: normal_distribution
clipbound: 5.0
forward_temperature: 0.88
max_grad_norm: 0.25
scale: 6
dpo_beta: 0.1
loss_type: 'DRPO'
learn_mu_parameters: false
mu_value_hidden_size: 64
mu_value_coef: 1.0