{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96095f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys, pathlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification,DebertaV2ForSequenceClassification, GPTNeoXForCausalLM\n",
    "from llm_blender.pair_ranker.pairrm import DebertaV2PairRM\n",
    "from transformers import DataCollatorWithPadding\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "LOCAL_TRL_PARENT = \"/workspace/Self_play_DRPO\"\n",
    "if LOCAL_TRL_PARENT not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_TRL_PARENT)\n",
    "\n",
    "    \n",
    "# now the import will use your local copy:\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    ModelConfig,\n",
    "    DRPOTrainer,\n",
    "    DRPOConfig,\n",
    ")\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "from trl.data_utils import apply_chat_template\n",
    "\n",
    "data_cache_path = \"/workspace/dataset\"\n",
    "model_cache_path = '/workspace/model_cache'\n",
    "ref_policy_path = \"Qwen/Qwen2.5-1.5B-Instruct\" \n",
    "#target_policy_path = \"Qwen/Qwen2.5-1.5B-Instruct\" \n",
    "target_policy_500 = '/workspace/Self_play_DRPO/self_play_drpo_code/output/hh/0/checkpoint-600'\n",
    "target_policy_1000 = '/workspace/Self_play_DRPO/self_play_drpo_code/output/hh/0/checkpoint-1000'\n",
    "dpo_policy_path = 'august66/ultrafeedback_20k_qwen_1.5b_dpo_model'\n",
    "#reward_model_path = 'cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr'\n",
    "ds_path = 'august66/drpo_hh_qwen2.5_1.5b'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f18437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, task = 'generation', model_type = 'decoder', model_cache_path =  '/workspace/model_cache'):\n",
    "\n",
    "    model_args = ModelConfig(model_path)\n",
    "    model_torch_dtype = (model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype))\n",
    "    model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "    padding_side = 'left' if model_type == 'decoder' else 'right'\n",
    "    truncation_side = 'left' if model_type == 'decoder' else 'right'\n",
    "\n",
    "    if task == 'generation':\n",
    "        model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "\n",
    "    elif task == 'reward':\n",
    "        model_instance = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "    \n",
    "\n",
    "    model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, \n",
    "        padding_side = padding_side, \n",
    "        truncation_side = truncation_side,\n",
    "        use_fast = True,\n",
    "        trust_remote_code = model_args.trust_remote_code,\n",
    "        cache_dir = model_cache_path\n",
    "    )\n",
    "\n",
    "    if model_tokenizer.pad_token is None:\n",
    "        model_tokenizer.pad_token = model_tokenizer.eos_token\n",
    "\n",
    "    if getattr(model_instance.config, \"pad_token_id\", None) is None:\n",
    "        model_instance.config.pad_token_id = model_tokenizer.pad_token_id\n",
    "\n",
    "    if model_tokenizer.eos_token is None:\n",
    "        model_tokenizer.eos_token = model_tokenizer.pad_token  \n",
    "\n",
    "    if getattr(model_instance.config, \"eos_token_id\", None) is None:\n",
    "        model_instance.config.eos_token_id = model_tokenizer.eos_token_id\n",
    "\n",
    "    return model_instance, model_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ab60c28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:51<00:00, 25.88s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 40.94it/s]\n"
     ]
    }
   ],
   "source": [
    "ref_model_instance, ref_model_tokenizer = load_model(ref_policy_path, task = 'generation')\n",
    "target_model_instance, target_model_tokenizer = load_model(target_policy_path, task = 'generation')\n",
    "dpo_policy_instance, dpo_policy_tokenizer = load_model(dpo_polict_path, task = 'generation')\n",
    "#reward_model_instance, reward_model_tokenizer = load_model(reward_model_path, task ='reward')\n",
    "\n",
    "ds = load_dataset(ds_path, cache_dir=data_cache_path, split = 'train')\n",
    "prompts = ds['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5543579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model_instance, model_tokenizer, prompts, \n",
    "                      temperature=0.0, max_new_tokens=256, \n",
    "                      n_responses=2, batch_size=8, device='cuda'):\n",
    "    generation_kwargs = {\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": True if temperature > 0 else False,\n",
    "        \"eos_token_id\": model_tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    model_instance.to(device)\n",
    "    all_prompt_responses = []\n",
    "\n",
    "    # loop over batches\n",
    "    for start in tqdm(range(0, len(prompts), batch_size)):\n",
    "        \n",
    "        batch_prompts = prompts[start:start+batch_size]\n",
    "        batch_prompts = [model_tokenizer.apply_chat_template(\n",
    "            p,\n",
    "            add_generation_prompt = True,\n",
    "            tokenize = False\n",
    "        ) for p in batch_prompts]\n",
    "        print (batch_prompts)\n",
    "        encoded_inputs = model_tokenizer(\n",
    "            batch_prompts, \n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "\n",
    "        prompt_lengths = encoded_inputs.attention_mask.sum(-1).tolist()\n",
    "\n",
    "        outputs = model_instance.generate(\n",
    "            **encoded_inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=n_responses,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "        generated_tokens = []\n",
    "        for i in range(len(batch_prompts)):\n",
    "            prompt_length = prompt_lengths[i]\n",
    "            for j in range(n_responses):\n",
    "                idx = i * n_responses + j\n",
    "                generated_tokens.append(outputs[idx][prompt_length:])\n",
    "\n",
    "        decoded_responses = model_tokenizer.batch_decode(\n",
    "            generated_tokens, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        for i, prompt in enumerate(batch_prompts):\n",
    "            for j in range(n_responses):\n",
    "                idx = i * n_responses + j\n",
    "                all_prompt_responses.append([prompt, decoded_responses[idx]])\n",
    "\n",
    "        del encoded_inputs, outputs, generated_tokens, decoded_responses\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_prompt_responses\n",
    "\n",
    "\n",
    "\n",
    "def get_reward_batch(\n",
    "    model_instance, \n",
    "    model_tokenizer, \n",
    "    inputs,                    # list[(prompt, response)]\n",
    "    device='cuda', \n",
    "    batch_size=32\n",
    "):\n",
    "    model_instance.to(device)\n",
    "    all_rewards = []\n",
    "\n",
    "    for start in tqdm(range(0, len(inputs), batch_size)):\n",
    "        batch_inputs = inputs[start:start+batch_size]\n",
    "\n",
    "        # concat prompt+response text\n",
    "        responses = [prompt + response for (prompt, response) in batch_inputs]\n",
    "\n",
    "        encoded_inputs = model_tokenizer(\n",
    "            responses,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_instance(**encoded_inputs)\n",
    "\n",
    "        # Typical reward heads return shape [B] or [B,1] in outputs.logits\n",
    "        rewards = outputs.logits.squeeze(-1).detach().cpu().tolist()\n",
    "        if isinstance(rewards, list):\n",
    "            all_rewards.extend(rewards)\n",
    "        else:\n",
    "            all_rewards.append(rewards)\n",
    "\n",
    "        del encoded_inputs, outputs\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_expected_kl(\n",
    "    ref_model,\n",
    "    tokenizer,                 # same tokenizer for both models\n",
    "    target_model,\n",
    "    inputs,                    # list[[prompt, response], ...]\n",
    "    device='cuda',\n",
    "    batch_size=8,\n",
    "    max_length=1024,\n",
    "    response_only=False,        # if True, only score response tokens\n",
    "):\n",
    "    ref_model.to(device).eval()\n",
    "    target_model.to(device).eval()\n",
    "\n",
    "    seq_diff_all = []\n",
    "\n",
    "    for start in tqdm(range(0, len(inputs), batch_size)):\n",
    "        batch_pairs = inputs[start:start+batch_size]\n",
    "        prompts  = [p for p, r in batch_pairs]\n",
    "        full_txt = [p + r for p, r in batch_pairs]   # concatenate exactly as specified\n",
    "\n",
    "        # Encode full sequences (prompt+response)\n",
    "        enc_full = tokenizer(\n",
    "            full_txt,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        # Encode prompts alone to get per-sample prompt token counts\n",
    "        enc_prompt = tokenizer(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        input_ids = enc_full[\"input_ids\"]        # [B, T]\n",
    "        attn_mask = enc_full[\"attention_mask\"]   # [B, T]\n",
    "        prompt_len = enc_prompt[\"attention_mask\"].sum(dim=1).to(torch.long)  # [B]\n",
    "\n",
    "        # Forward both models\n",
    "        ref_logits    = ref_model(input_ids=input_ids,    attention_mask=attn_mask).logits   # [B, T, V]\n",
    "        target_logits = target_model(input_ids=input_ids, attention_mask=attn_mask).logits   # [B, T, V]\n",
    "\n",
    "        # Teacher-forced next-token logprobs\n",
    "        ref_logp = F.log_softmax(ref_logits[:, :-1, :], dim=-1)      # [B, T-1, V]\n",
    "        tgt_logp = F.log_softmax(target_logits[:, :-1, :], dim=-1)   # [B, T-1, V]\n",
    "        tgt_p = tgt_logp.exp()\n",
    "        labels   = input_ids[:, 1:]                                   # [B, T-1]\n",
    "        mask     = attn_mask[:, 1:].to(ref_logp.dtype).clone()        # [B, T-1]\n",
    "\n",
    "        # If we only want the response, drop prompt contributions per sample.\n",
    "        if response_only:\n",
    "            # After shifting, tokens with position < (prompt_len - 1) belong to the prompt\n",
    "            cuts = (prompt_len - 1).clamp_min(0)                      # [B]\n",
    "            # Zero out up to 'cut' for each row\n",
    "            # Note: sequences are padded; mask already 0 on pad positions\n",
    "            for i, cut in enumerate(cuts.tolist()):\n",
    "                if cut > 0:\n",
    "                    cut = min(cut, mask.shape[1])  # guard against extreme lengths\n",
    "                    mask[i, :cut] = 0.0\n",
    "\n",
    "\n",
    "        per_token = (tgt_p * (tgt_logp - ref_logp)).sum(dim = -1)\n",
    "        diff = (per_token*mask).sum(dim = -1)\n",
    "\n",
    "        seq_diff_all.append(diff.detach().cpu())\n",
    "\n",
    "        # cleanup\n",
    "        del enc_full, enc_prompt, ref_logits, target_logits, ref_logp, tgt_logp, tgt_p, labels, mask\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    diff_list = torch.cat(seq_diff_all).tolist()\n",
    "\n",
    "    \n",
    "    return diff_list\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e553b66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWhy does water become solid when it is boiling?<|im_end|>\\n<|im_start|>assistant\\n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.76s/it]\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.76s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "temperature = [1.0]\n",
    "total = 10\n",
    "step = 10\n",
    "prompt = ds['prompt'][10000]\n",
    "if not isinstance(prompt, list) or not isinstance(prompt[0], list):\n",
    "    prompt = [prompt]\n",
    "logp_diff_dict = {}\n",
    "for t in temperature:\n",
    "    outputs = []\n",
    "    for i in tqdm(range(0, total, step)):\n",
    "        output = generate_response(target_model_instance, target_model_tokenizer, prompt, n_responses=step, temperature = t)\n",
    "        outputs.extend(output)\n",
    "    logp_diff = get_expected_kl(\n",
    "            ref_model_instance, ref_model_tokenizer, target_model_instance, outputs, response_only = True\n",
    "    )\n",
    "    logp_diff_dict[t] = logp_diff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b03f8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_kl_divergence(target_model, ref_model, prompt_ids, prompt_attention_mask, completion_ids, completion_attention_mask, temperature=1.0):\n",
    "\n",
    "    target_model.to('cuda')\n",
    "    ref_model.to('cuda')\n",
    "    max_length = 1024\n",
    "    # Get the number of tokens to truncate from prompt\n",
    "    num_tokens_to_truncate = max(prompt_ids.size(1) + completion_ids.size(1) - max_length, 0)\n",
    "\n",
    "    # Truncate left to avoid oom\n",
    "    prompt_ids = prompt_ids[:, num_tokens_to_truncate:]\n",
    "    prompt_attention_mask = prompt_attention_mask[:, num_tokens_to_truncate:]\n",
    "\n",
    "    # Concat the prompt and completion\n",
    "    prompt_completion_ids = torch.cat((prompt_ids, completion_ids), dim=1)\n",
    "    prompt_completion_mask = torch.cat((prompt_attention_mask, completion_attention_mask), dim=1)\n",
    "\n",
    "    # Get the logps of the completions from the model\n",
    "    logits_target = target_model(prompt_completion_ids, attention_mask=prompt_completion_mask).logits\n",
    "    logits_ref = ref_model(prompt_completion_ids, attention_mask=prompt_completion_mask).logits\n",
    "\n",
    "    prompt_length = prompt_ids.size(1)\n",
    "    logits_completion_target = logits_target[:, max(0, prompt_length - 1) : -1, :]\n",
    "    logits_completion_ref = logits_ref[:, max(0, prompt_length - 1) : -1, :]\n",
    "\n",
    "    if completion_ids.size(1) > logits_target.size(1):\n",
    "        completion_ids = completion_ids[:, : logits_target.size(1)]\n",
    "\n",
    "    mask = completion_attention_mask[:, 1:].to(torch.float32).clone()\n",
    "    logp_completion_target = F.log_softmax(logits_completion_target / (temperature + 1e-7), dim=-1)\n",
    "    logp_completion_ref = F.log_softmax(logits_completion_ref / (temperature + 1e-7), dim=-1)\n",
    "    p_target= logp_completion_target.exp()\n",
    "\n",
    "    kl_divergence = (p_target * (logp_completion_target - logp_completion_ref) * mask.unsqueeze(-1)).sum(-1).sum(-1)\n",
    "\n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6004b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = outputs[0][0]\n",
    "completion = outputs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ff167",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_prompts = target_model_tokenizer(\n",
    "            prompt,\n",
    "            padding='max_length',\n",
    "            max_length = 1024,\n",
    "            return_tensors='pt'\n",
    ").to('cuda')\n",
    "encoded_completions = target_model_tokenizer(\n",
    "            completion,\n",
    "            padding='max_length',\n",
    "            max_length = 1024, \n",
    "            return_tensors='pt'\n",
    ").to('cuda')\n",
    "prompt_id = encoded_prompts['input_ids']\n",
    "prompt_attention_mask = encoded_prompts['attention_mask']\n",
    "completion_id = encoded_completions['input_ids']    \n",
    "completion_attention_mask = encoded_completions['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fa7f4b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.4464], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_calculate_kl_divergence(target_model_instance, dpo_policy_instance, prompt_id, prompt_attention_mask, completion_id, completion_attention_mask, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61337627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
