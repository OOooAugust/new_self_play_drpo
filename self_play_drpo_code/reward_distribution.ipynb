{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96095f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys, pathlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification,DebertaV2ForSequenceClassification, GPTNeoXForCausalLM\n",
    "from llm_blender.pair_ranker.pairrm import DebertaV2PairRM\n",
    "from transformers import DataCollatorWithPadding\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "LOCAL_TRL_PARENT = \"/workspace/Self_play_DRPO\"\n",
    "if LOCAL_TRL_PARENT not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_TRL_PARENT)\n",
    "\n",
    "    \n",
    "# now the import will use your local copy:\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    ModelConfig,\n",
    "    DRPOTrainer,\n",
    "    DRPOConfig,\n",
    ")\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "from trl.data_utils import apply_chat_template\n",
    "data_cache_path = \"/workspace/dataset\"\n",
    "model_cache_path = '/workspace/model_cache'\n",
    "ref_policy_path = 'cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr'\n",
    "target_policy_path = 'cleanrl/EleutherAI_pythia-1b-deduped__ppo__tldr'\n",
    "reward_model_path = 'cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr'\n",
    "ds_path = 'august66/drpo_ultrafeedback_qwen2.5-1.5b_first_iter_20k'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f18437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, task = 'generation', model_type = 'decoder', model_cache_path =  '/workspace/model_cache'):\n",
    "\n",
    "    model_args = ModelConfig(model_path)\n",
    "    model_torch_dtype = (model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype))\n",
    "    model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "    padding_side = 'left' if model_type == 'decoder' else 'right'\n",
    "    truncation_side = 'left' if model_type == 'decoder' else 'right'\n",
    "\n",
    "    if task == 'generation':\n",
    "        model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "\n",
    "    elif task == 'reward':\n",
    "        model_instance = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "    \n",
    "\n",
    "    model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, \n",
    "        padding_side = padding_side, \n",
    "        truncation_side = truncation_side,\n",
    "        use_fast = True,\n",
    "        trust_remote_code = model_args.trust_remote_code,\n",
    "        cache_dir = model_cache_path\n",
    "    )\n",
    "\n",
    "    if model_tokenizer.pad_token is None:\n",
    "        model_tokenizer.pad_token = model_tokenizer.eos_token\n",
    "\n",
    "    if getattr(model_instance.config, \"pad_token_id\", None) is None:\n",
    "        model_instance.config.pad_token_id = model_tokenizer.pad_token_id\n",
    "\n",
    "    if model_tokenizer.eos_token is None:\n",
    "        model_tokenizer.eos_token = model_tokenizer.pad_token  \n",
    "\n",
    "    if getattr(model_instance.config, \"eos_token_id\", None) is None:\n",
    "        model_instance.config.eos_token_id = model_tokenizer.eos_token_id\n",
    "\n",
    "    return model_instance, model_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab60c28a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ref_policy_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m ref_model_instance, ref_model_tokenizer = load_model(\u001b[43mref_policy_path\u001b[49m, task = \u001b[33m'\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m target_model_instance, target_model_tokenizer = load_model(target_policy_path, task = \u001b[33m'\u001b[39m\u001b[33mgeneration\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#reward_model_instance, reward_model_tokenizer = load_model(reward_model_path, task ='reward')\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'ref_policy_path' is not defined"
     ]
    }
   ],
   "source": [
    "ref_model_instance, ref_model_tokenizer = load_model(ref_policy_path, task = 'generation')\n",
    "target_model_instance, target_model_tokenizer = load_model(target_policy_path, task = 'generation')\n",
    "#reward_model_instance, reward_model_tokenizer = load_model(reward_model_path, task ='reward')\n",
    "\n",
    "ds = load_dataset(ds_path, cache_dir=data_cache_path, split = 'train')\n",
    "prompts = ds['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5543579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model_instance, model_tokenizer, prompts, \n",
    "                      temperature=0.0, max_new_tokens=256, \n",
    "                      n_responses=2, batch_size=8, device='cuda'):\n",
    "    generation_kwargs = {\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": True if temperature > 0 else False,\n",
    "        \"eos_token_id\": model_tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    model_instance.to(device)\n",
    "    all_prompt_responses = []\n",
    "\n",
    "    # loop over batches\n",
    "    for start in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch_prompts = prompts[start:start+batch_size]\n",
    "\n",
    "        encoded_inputs = model_tokenizer(\n",
    "            batch_prompts, \n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        prompt_lengths = encoded_inputs.attention_mask.sum(-1).tolist()\n",
    "\n",
    "        outputs = model_instance.generate(\n",
    "            **encoded_inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=n_responses,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "        generated_tokens = []\n",
    "        for i in range(len(batch_prompts)):\n",
    "            prompt_length = prompt_lengths[i]\n",
    "            for j in range(n_responses):\n",
    "                idx = i * n_responses + j\n",
    "                generated_tokens.append(outputs[idx][prompt_length:])\n",
    "\n",
    "        decoded_responses = model_tokenizer.batch_decode(\n",
    "            generated_tokens, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        for i, prompt in enumerate(batch_prompts):\n",
    "            for j in range(n_responses):\n",
    "                idx = i * n_responses + j\n",
    "                all_prompt_responses.append([prompt, decoded_responses[idx]])\n",
    "\n",
    "        del encoded_inputs, outputs, generated_tokens, decoded_responses\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_prompt_responses\n",
    "\n",
    "\n",
    "\n",
    "def get_reward_batch(\n",
    "    model_instance, \n",
    "    model_tokenizer, \n",
    "    inputs,                    # list[(prompt, response)]\n",
    "    device='cuda', \n",
    "    batch_size=32\n",
    "):\n",
    "    model_instance.to(device)\n",
    "    all_rewards = []\n",
    "\n",
    "    for start in tqdm(range(0, len(inputs), batch_size)):\n",
    "        batch_inputs = inputs[start:start+batch_size]\n",
    "\n",
    "        # concat prompt+response text\n",
    "        responses = [prompt + response for (prompt, response) in batch_inputs]\n",
    "\n",
    "        encoded_inputs = model_tokenizer(\n",
    "            responses,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_instance(**encoded_inputs)\n",
    "\n",
    "        # Typical reward heads return shape [B] or [B,1] in outputs.logits\n",
    "        rewards = outputs.logits.squeeze(-1).detach().cpu().tolist()\n",
    "        if isinstance(rewards, list):\n",
    "            all_rewards.extend(rewards)\n",
    "        else:\n",
    "            all_rewards.append(rewards)\n",
    "\n",
    "        del encoded_inputs, outputs\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_expected_kl(\n",
    "    ref_model,\n",
    "    tokenizer,                 # same tokenizer for both models\n",
    "    target_model,\n",
    "    inputs,                    # list[[prompt, response], ...]\n",
    "    device='cuda',\n",
    "    batch_size=8,\n",
    "    max_length=1024,\n",
    "    response_only=False,        # if True, only score response tokens\n",
    "):\n",
    "    ref_model.to(device).eval()\n",
    "    target_model.to(device).eval()\n",
    "\n",
    "    seq_diff_all = []\n",
    "\n",
    "    for start in tqdm(range(0, len(inputs), batch_size)):\n",
    "        batch_pairs = inputs[start:start+batch_size]\n",
    "        prompts  = [p for p, r in batch_pairs]\n",
    "        full_txt = [p + r for p, r in batch_pairs]   # concatenate exactly as specified\n",
    "\n",
    "        # Encode full sequences (prompt+response)\n",
    "        enc_full = tokenizer(\n",
    "            full_txt,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        # Encode prompts alone to get per-sample prompt token counts\n",
    "        enc_prompt = tokenizer(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        input_ids = enc_full[\"input_ids\"]        # [B, T]\n",
    "        attn_mask = enc_full[\"attention_mask\"]   # [B, T]\n",
    "        prompt_len = enc_prompt[\"attention_mask\"].sum(dim=1).to(torch.long)  # [B]\n",
    "\n",
    "        # Forward both models\n",
    "        ref_logits    = ref_model(input_ids=input_ids,    attention_mask=attn_mask).logits   # [B, T, V]\n",
    "        target_logits = target_model(input_ids=input_ids, attention_mask=attn_mask).logits   # [B, T, V]\n",
    "\n",
    "        # Teacher-forced next-token logprobs\n",
    "        ref_logp = F.log_softmax(ref_logits[:, :-1, :], dim=-1)      # [B, T-1, V]\n",
    "        tgt_logp = F.log_softmax(target_logits[:, :-1, :], dim=-1)   # [B, T-1, V]\n",
    "        tgt_p = tgt_logp.exp()\n",
    "        labels   = input_ids[:, 1:]                                   # [B, T-1]\n",
    "        mask     = attn_mask[:, 1:].to(ref_logp.dtype).clone()        # [B, T-1]\n",
    "\n",
    "        # If we only want the response, drop prompt contributions per sample.\n",
    "        if response_only:\n",
    "            # After shifting, tokens with position < (prompt_len - 1) belong to the prompt\n",
    "            cuts = (prompt_len - 1).clamp_min(0)                      # [B]\n",
    "            # Zero out up to 'cut' for each row\n",
    "            # Note: sequences are padded; mask already 0 on pad positions\n",
    "            for i, cut in enumerate(cuts.tolist()):\n",
    "                if cut > 0:\n",
    "                    cut = min(cut, mask.shape[1])  # guard against extreme lengths\n",
    "                    mask[i, :cut] = 0.0\n",
    "\n",
    "\n",
    "        per_token = (tgt_p * (tgt_logp - ref_logp)).sum(dim = -1)\n",
    "        diff = (per_token*mask).sum(dim = -1)\n",
    "\n",
    "        seq_diff_all.append(diff.detach().cpu())\n",
    "\n",
    "        # cleanup\n",
    "        del enc_full, enc_prompt, ref_logits, target_logits, ref_logp, tgt_logp, tgt_p, labels, mask\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    diff_list = torch.cat(seq_diff_all).tolist()\n",
    "\n",
    "    \n",
    "    return diff_list\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e553b66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m outputs = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, total, step)):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     output = \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_model_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_responses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     outputs.extend(output)\n\u001b[32m     11\u001b[39m logp_diff = get_expected_kl(\n\u001b[32m     12\u001b[39m         ref_model_instance, ref_model_tokenizer, target_model_instance, outputs, response_only = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(model_instance, model_tokenizer, prompts, temperature, max_new_tokens, n_responses, batch_size, device)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(prompts), batch_size)):\n\u001b[32m     20\u001b[39m     batch_prompts = prompts[start:start+batch_size]\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     encoded_inputs = \u001b[43mmodel_tokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     30\u001b[39m     prompt_lengths = encoded_inputs.attention_mask.sum(-\u001b[32m1\u001b[39m).tolist()\n\u001b[32m     32\u001b[39m     outputs = model_instance.generate(\n\u001b[32m     33\u001b[39m         **encoded_inputs,\n\u001b[32m     34\u001b[39m         max_new_tokens=max_new_tokens,\n\u001b[32m     35\u001b[39m         num_return_sequences=n_responses,\n\u001b[32m     36\u001b[39m         **generation_kwargs\n\u001b[32m     37\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2910\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2908\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2909\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2910\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2911\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2912\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2970\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2967\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2969\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[32m-> \u001b[39m\u001b[32m2970\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor `list[list[str]]` (batch of pretokenized examples).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m     )\n\u001b[32m   2975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[32m   2976\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2977\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2978\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor `list[list[str]]` (batch of pretokenized examples).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2979\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "temperature = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "total = 100\n",
    "step = 100\n",
    "prompt = ds['prompt'][10000]\n",
    "logp_diff_dict = {}\n",
    "for t in temperature:\n",
    "    outputs = []\n",
    "    for i in tqdm(range(0, total, step)):\n",
    "        output = generate_response(target_model_instance, target_model_tokenizer, prompt, n_responses=step, temperature = t)\n",
    "        outputs.extend(output)\n",
    "    logp_diff = get_expected_kl(\n",
    "            ref_model_instance, ref_model_tokenizer, target_model_instance, outputs, response_only = True\n",
    "    )\n",
    "    logp_diff_dict[t] = logp_diff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "018b1abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1, Mean: 19.97113744735718, Std: 1.588950585588776\n",
      "Temperature: 0.3, Mean: 19.53560682296753, Std: 2.6689517989744367\n",
      "Temperature: 0.5, Mean: 19.123689908981323, Std: 2.6595066874772963\n",
      "Temperature: 0.7, Mean: 21.255804958343507, Std: 2.8316154150023487\n",
      "Temperature: 1.0, Mean: 21.43497528076172, Std: 2.7243864094601755\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "for t in temperature:\n",
    "    logp_diff = logp_diff_dict[t]\n",
    "    logp_diff_mean = np.mean(logp_diff)\n",
    "    logp_diff_std = np.std(logp_diff)\n",
    "    print (f\"Temperature: {t}, Mean: {logp_diff_mean}, Std: {logp_diff_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612d655a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Why does water become solid when it is boiling?',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa9ab1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': \"**Customer:** Good day! I'm calling to report an issue with my recent purchase.\\n\\n**Agent:** Hello! Thank you for reaching out. How can I assist you today?\\n\\n**Customer:** My order has not arrived yet, and it's been two weeks since I placed the order.\\n\\n**Agent:** I understand that this is concerning. Could you please provide me with your order number so we can look into it further?\\n\\n**Customer:** Sure thing. (Repeats their order number)\\n\\n**Agent:** Thank you. Let’s proceed with checking on your order status. According to our system, it appears your package was delayed due to unexpected shipping issues. Is there anything else I should know about your situation? \\n\\n**Customer:** Well, yes... The item I ordered is quite important to me, and I’m really concerned about its delivery time. Can we expedite it somehow or at least give me an estimated date of arrival?\\n\\n**Agent:** We do have some options available to help speed up the process while ensuring your privacy and security remain protected. Would you like to proceed with an expedited shipping option now, or would you prefer another method to track the progress of your order until it arrives safely?\\n\\n**Customer:** Honestly, any action you can take will be greatly appreciated. I just want to get this shipped as soon as possible.\\n\\n**Agent:** Understood. To expedite your shipment, we’ll need to pay an additional fee. Please confirm if you’re comfortable with this charge before we proceed.\\n\\n**Customer:** Yes, I agree that paying the extra fee is necessary given the circumstances. Will I still receive full coverage and support in case something goes wrong during the expedited shipping process?\\n\\n**Agent:** Absolutely. Your satisfaction is our priority, and we will ensure all steps taken to expedite your order comply with our standard policies. Once the transaction is completed, one of our team members will follow up with you regarding tracking information and updates throughout the shipping process. \\n\\n**Customer:** That sounds good. I appreciate your assistance.\\n\\n**Agent:** Thank you for choosing us to serve you. If there’s anything else I can help you with, don’t hesitate to reach out. Have a great rest of your day!\\n\\n**Customer:** Same here. Thanks again for taking care of this quickly.\\n\\n**Agent:** You're welcome. Take care!\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['a1'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f8e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
