{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96095f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/miniconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys, pathlib\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification,DebertaV2ForSequenceClassification, GPTNeoXForCausalLM\n",
    "from llm_blender.pair_ranker.pairrm import DebertaV2PairRM\n",
    "from transformers import DataCollatorWithPadding\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "LOCAL_TRL_PARENT = \"/workspace/Self_play_DRPO\"\n",
    "if LOCAL_TRL_PARENT not in sys.path:\n",
    "    sys.path.insert(0, LOCAL_TRL_PARENT)\n",
    "\n",
    "    \n",
    "# now the import will use your local copy:\n",
    "from trl import (\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    ModelConfig,\n",
    "    DRPOTrainer,\n",
    "    DRPOConfig,\n",
    ")\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n",
    "from trl.data_utils import apply_chat_template\n",
    "data_cache_path = \"/workspace/dataset\"\n",
    "model_cache_path = '/workspace/model_cache'\n",
    "ref_policy_path = 'cleanrl/EleutherAI_pythia-1b-deduped__sft__tldr'\n",
    "target_policy_path = 'cleanrl/EleutherAI_pythia-1b-deduped__ppo__tldr'\n",
    "reward_model_path = 'cleanrl/EleutherAI_pythia-1b-deduped__reward__tldr'\n",
    "ds_path = 'Kyleyee/train_data_tldr'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f18437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, task = 'generation', model_type = 'decoder', model_cache_path =  '/workspace/model_cache'):\n",
    "\n",
    "    model_args = ModelConfig(model_path)\n",
    "    model_torch_dtype = (model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype))\n",
    "    model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = model_torch_dtype, \n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    "    )\n",
    "\n",
    "    padding_side = 'left' if model_type == 'decoder' else 'right'\n",
    "    truncation_side = 'left' if model_type == 'decoder' else 'right'\n",
    "\n",
    "    if task == 'generation':\n",
    "        model_instance = AutoModelForCausalLM.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "\n",
    "    elif task == 'reward':\n",
    "        model_instance = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_args.model_name_or_path,\n",
    "            **model_kwargs,\n",
    "            cache_dir = model_cache_path,\n",
    "        )\n",
    "    \n",
    "\n",
    "    model_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path, \n",
    "        padding_side = padding_side, \n",
    "        truncation_side = truncation_side,\n",
    "        use_fast = True,\n",
    "        trust_remote_code = model_args.trust_remote_code,\n",
    "        cache_dir = model_cache_path\n",
    "    )\n",
    "\n",
    "    if model_tokenizer.pad_token is None:\n",
    "        model_tokenizer.pad_token = model_tokenizer.eos_token\n",
    "\n",
    "    if getattr(model_instance.config, \"pad_token_id\", None) is None:\n",
    "        model_instance.config.pad_token_id = model_tokenizer.pad_token_id\n",
    "\n",
    "    if model_tokenizer.eos_token is None:\n",
    "        model_tokenizer.eos_token = model_tokenizer.pad_token  \n",
    "\n",
    "    if getattr(model_instance.config, \"eos_token_id\", None) is None:\n",
    "        model_instance.config.eos_token_id = model_tokenizer.eos_token_id\n",
    "\n",
    "    return model_instance, model_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab60c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model_instance, ref_model_tokenizer = load_model(ref_policy_path, task = 'generation')\n",
    "target_model_instance, target_model_tokenizer = load_model(target_policy_path, task = 'generation')\n",
    "reward_model_instance, reward_model_tokenizer = load_model(reward_model_path, task ='reward')\n",
    "\n",
    "ds = load_dataset(ds_path, cache_dir=data_cache_path, split = 'train')\n",
    "prompts = ds['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5543579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model_instance, model_tokenizer, prompts, \n",
    "                      temperature=0.0, max_new_tokens=256, \n",
    "                      n_responses=2, batch_size=8, device='cuda'):\n",
    "    generation_kwargs = {\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": temperature,\n",
    "        \"do_sample\": True if temperature > 0 else False,\n",
    "        \"eos_token_id\": model_tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    model_instance.to(device)\n",
    "    all_prompt_responses = []\n",
    "\n",
    "    # loop over batches\n",
    "    for start in tqdm(range(0, len(prompts), batch_size)):\n",
    "        batch_prompts = prompts[start:start+batch_size]\n",
    "\n",
    "        encoded_inputs = model_tokenizer(\n",
    "            batch_prompts, \n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        prompt_lengths = encoded_inputs.attention_mask.sum(-1).tolist()\n",
    "\n",
    "        outputs = model_instance.generate(\n",
    "            **encoded_inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_return_sequences=n_responses,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "\n",
    "        generated_tokens = []\n",
    "        for i in range(len(batch_prompts)):\n",
    "            prompt_length = prompt_lengths[i]\n",
    "            for j in range(n_responses):\n",
    "                idx = i * n_responses + j\n",
    "                generated_tokens.append(outputs[idx][prompt_length:])\n",
    "\n",
    "        decoded_responses = model_tokenizer.batch_decode(\n",
    "            generated_tokens, skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        for i, prompt in enumerate(batch_prompts):\n",
    "            for j in range(n_responses):\n",
    "                idx = i * n_responses + j\n",
    "                all_prompt_responses.append([prompt, decoded_responses[idx]])\n",
    "\n",
    "        del encoded_inputs, outputs, generated_tokens, decoded_responses\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_prompt_responses\n",
    "\n",
    "\n",
    "\n",
    "def get_reward_batch(\n",
    "    model_instance, \n",
    "    model_tokenizer, \n",
    "    inputs,                    # list[(prompt, response)]\n",
    "    device='cuda', \n",
    "    batch_size=32\n",
    "):\n",
    "    model_instance.to(device)\n",
    "    all_rewards = []\n",
    "\n",
    "    for start in tqdm(range(0, len(inputs), batch_size)):\n",
    "        batch_inputs = inputs[start:start+batch_size]\n",
    "\n",
    "        # concat prompt+response text\n",
    "        responses = [prompt + response for (prompt, response) in batch_inputs]\n",
    "\n",
    "        encoded_inputs = model_tokenizer(\n",
    "            responses,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_instance(**encoded_inputs)\n",
    "\n",
    "        # Typical reward heads return shape [B] or [B,1] in outputs.logits\n",
    "        rewards = outputs.logits.squeeze(-1).detach().cpu().tolist()\n",
    "        if isinstance(rewards, list):\n",
    "            all_rewards.extend(rewards)\n",
    "        else:\n",
    "            all_rewards.append(rewards)\n",
    "\n",
    "        del encoded_inputs, outputs\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return all_rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_expected_kl(\n",
    "    ref_model,\n",
    "    tokenizer,                 # same tokenizer for both models\n",
    "    target_model,\n",
    "    inputs,                    # list[[prompt, response], ...]\n",
    "    device='cuda',\n",
    "    batch_size=8,\n",
    "    max_length=1024,\n",
    "    response_only=False,        # if True, only score response tokens\n",
    "):\n",
    "    ref_model.to(device).eval()\n",
    "    target_model.to(device).eval()\n",
    "\n",
    "    seq_diff_all = []\n",
    "\n",
    "    for start in tqdm(range(0, len(inputs), batch_size)):\n",
    "        batch_pairs = inputs[start:start+batch_size]\n",
    "        prompts  = [p for p, r in batch_pairs]\n",
    "        full_txt = [p + r for p, r in batch_pairs]   # concatenate exactly as specified\n",
    "\n",
    "        # Encode full sequences (prompt+response)\n",
    "        enc_full = tokenizer(\n",
    "            full_txt,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        # Encode prompts alone to get per-sample prompt token counts\n",
    "        enc_prompt = tokenizer(\n",
    "            prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        input_ids = enc_full[\"input_ids\"]        # [B, T]\n",
    "        attn_mask = enc_full[\"attention_mask\"]   # [B, T]\n",
    "        prompt_len = enc_prompt[\"attention_mask\"].sum(dim=1).to(torch.long)  # [B]\n",
    "\n",
    "        # Forward both models\n",
    "        ref_logits    = ref_model(input_ids=input_ids,    attention_mask=attn_mask).logits   # [B, T, V]\n",
    "        target_logits = target_model(input_ids=input_ids, attention_mask=attn_mask).logits   # [B, T, V]\n",
    "\n",
    "        # Teacher-forced next-token logprobs\n",
    "        ref_logp = F.log_softmax(ref_logits[:, :-1, :], dim=-1)      # [B, T-1, V]\n",
    "        tgt_logp = F.log_softmax(target_logits[:, :-1, :], dim=-1)   # [B, T-1, V]\n",
    "        labels   = input_ids[:, 1:]                                   # [B, T-1]\n",
    "        mask     = attn_mask[:, 1:].to(ref_logp.dtype).clone()        # [B, T-1]\n",
    "\n",
    "        # If we only want the response, drop prompt contributions per sample.\n",
    "        if response_only:\n",
    "            # After shifting, tokens with position < (prompt_len - 1) belong to the prompt\n",
    "            cuts = (prompt_len - 1).clamp_min(0)                      # [B]\n",
    "            # Zero out up to 'cut' for each row\n",
    "            # Note: sequences are padded; mask already 0 on pad positions\n",
    "            for i, cut in enumerate(cuts.tolist()):\n",
    "                if cut > 0:\n",
    "                    cut = min(cut, mask.shape[1])  # guard against extreme lengths\n",
    "                    mask[i, :cut] = 0.0\n",
    "\n",
    "        # Gather log-prob at the *label* token only (no full-vocab sums)\n",
    "        ref_tok_lp = ref_logp.gather(-1, labels.unsqueeze(-1)).squeeze(-1)  # [B, T-1]\n",
    "        tgt_tok_lp = tgt_logp.gather(-1, labels.unsqueeze(-1)).squeeze(-1)  # [B, T-1]\n",
    "        tgt_tok_p = tgt_tok_lp.exp()\n",
    "\n",
    "        diff = (tgt_tok_p * (tgt_tok_lp - ref_tok_lp) * mask).sum(dim = 1)\n",
    "\n",
    "        seq_diff_all.append(diff.detach().cpu())\n",
    "\n",
    "        # cleanup\n",
    "        del enc_full, enc_prompt, ref_logits, target_logits, ref_logp, tgt_logp, labels, mask, ref_tok_lp, tgt_tok_lp\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    diff_list = torch.cat(seq_diff_all).tolist()\n",
    "\n",
    "    \n",
    "    return diff_list\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553b66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.26s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.27s/it]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.02s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.14s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.02s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.26s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.32s/it]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.03s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.19s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.23s/it]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.04s/it]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.22s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.24s/it]\n",
      "100%|██████████| 13/13 [00:13<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "temperature = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "total = 2000\n",
    "step = 100\n",
    "prompt = ds['prompt'][4000]\n",
    "logp_diff_dict = {}\n",
    "for t in temperature:\n",
    "    outputs = []\n",
    "    for i in tqdm(range(0, total, step)):\n",
    "        output = generate_response(target_model_instance, target_model_tokenizer, prompt, n_responses=step, temperature = t)\n",
    "        outputs.extend(output)\n",
    "    logp_diff = get_expected_kl(\n",
    "            ref_model_instance, ref_model_tokenizer, target_model_instance, outputs, response_only = True\n",
    "    )\n",
    "    logp_diff_dict[t] = logp_diff\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "018b1abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1, Mean: 15.33936752319336, Std: 0.22643400689355653\n",
      "Temperature: 0.3, Mean: 14.61276858329773, Std: 1.2894942239680651\n",
      "Temperature: 0.5, Mean: 14.034057779312134, Std: 1.7655502985203773\n",
      "Temperature: 0.7, Mean: 13.420461387634278, Std: 2.2982169824421557\n",
      "Temperature: 1.0, Mean: 12.23510947227478, Std: 2.4929007388465756\n"
     ]
    }
   ],
   "source": [
    "for t in temperature:\n",
    "    logp_diff = logp_diff_dict[t]\n",
    "    logp_diff_mean = np.mean(logp_diff)\n",
    "    logp_diff_std = np.std(logp_diff)\n",
    "    print (f\"Temperature: {t}, Mean: {logp_diff_mean}, Std: {logp_diff_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42edecb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1, Mean: 13.964138650894165, Std: 2.047489847830732\n",
      "Temperature: 0.3, Mean: 13.624027314186096, Std: 2.949803264829852\n",
      "Temperature: 0.5, Mean: 12.73402428150177, Std: 3.2580189284055123\n",
      "Temperature: 0.7, Mean: 12.185903851985932, Std: 3.492453841257258\n",
      "Temperature: 1.0, Mean: 11.272922506332398, Std: 3.265552759650484\n"
     ]
    }
   ],
   "source": [
    "for t in temperature:\n",
    "    logp_diff = logp_diff_dict[t]\n",
    "    logp_diff_mean = np.mean(logp_diff)\n",
    "    logp_diff_std = np.std(logp_diff)\n",
    "    print (f\"Temperature: {t}, Mean: {logp_diff_mean}, Std: {logp_diff_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d655a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
